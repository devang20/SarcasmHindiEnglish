{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "598be367",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-01 12:30:44.634619: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-05-01 12:30:44.634655: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "[nltk_data] Downloading package punkt to /home/user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "from collections import Counter\n",
    "import emoji\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from gensim.models import FastText\n",
    "from gensim.models import KeyedVectors\n",
    "#from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "#from tqdm import tqdm\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "#import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.data_utils import pad_sequences\n",
    "#from keras.initializers import Constant\n",
    "from keras.layers import ReLU\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Embedding, Input, LSTM, Bidirectional, TimeDistributed, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras import Model\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import math\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d797f62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['tweet_id', 'tweet_text', 'retweet_count', 'favorite_count',\n",
      "       'user_name', 'user_id', 'user_description', 'user_verified',\n",
      "       'user_followers_count', 'user_friends_count', 'user_listed_count',\n",
      "       'user_favourites_count', 'user_statuses_count', 'user_created_at'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@ajuonline #sarcasm Haal chaal thik thak hai-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@I_Am_Hollywood - Charlie Haas? #sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chetan Bhagat #epicmale #sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Bo_jasim86 LOOL 7atheer affaa 3allaik rgoobti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@FENUSF tees2al 3anek el 3afiaa 7abeeb galbe ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88586</th>\n",
       "      <td>@Abhi_4_Nation @narendramodi @PrakashJavdekar ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88587</th>\n",
       "      <td>Weh pehle-humari-pocket-se-SIRF-THODA-SA-nikal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88588</th>\n",
       "      <td>@rssurjewala Waqt ayega ki kuch janwar sirf zo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88589</th>\n",
       "      <td>@LawBharath @LEDtvn Bakhton ko pata nahi hoga ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88590</th>\n",
       "      <td>Sirf khud se piyar karo, maze me raho ge I swear.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88591 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweet_text\n",
       "0      @ajuonline #sarcasm Haal chaal thik thak hai-s...\n",
       "1               @I_Am_Hollywood - Charlie Haas? #sarcasm\n",
       "2                       Chetan Bhagat #epicmale #sarcasm\n",
       "3      @Bo_jasim86 LOOL 7atheer affaa 3allaik rgoobti...\n",
       "4      @FENUSF tees2al 3anek el 3afiaa 7abeeb galbe ,...\n",
       "...                                                  ...\n",
       "88586  @Abhi_4_Nation @narendramodi @PrakashJavdekar ...\n",
       "88587  Weh pehle-humari-pocket-se-SIRF-THODA-SA-nikal...\n",
       "88588  @rssurjewala Waqt ayega ki kuch janwar sirf zo...\n",
       "88589  @LawBharath @LEDtvn Bakhton ko pata nahi hoga ...\n",
       "88590  Sirf khud se piyar karo, maze me raho ge I swear.\n",
       "\n",
       "[88591 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_pickle('dataset/hinglish_tweet_ids_part1.pkl')\n",
    "print(df1.columns)\n",
    "df1[['tweet_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e66eab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['tweet_id', 'tweet_text', 'retweet_count', 'favorite_count',\n",
      "       'user_name', 'user_id', 'user_description', 'user_verified',\n",
      "       'user_followers_count', 'user_friends_count', 'user_listed_count',\n",
      "       'user_favourites_count', 'user_statuses_count', 'user_created_at'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@ajuonline #sarcasm Haal chaal thik thak hai-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@I_Am_Hollywood - Charlie Haas? #sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chetan Bhagat #epicmale #sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Bo_jasim86 LOOL 7atheer affaa 3allaik rgoobti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@FENUSF tees2al 3anek el 3afiaa 7abeeb galbe ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105192</th>\n",
       "      <td>\"Kaali kaali khaali raaton se Hone lagi hai do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105193</th>\n",
       "      <td>Jao pehle usese jakar puchho jisne meri Jameen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105194</th>\n",
       "      <td>Hai tu agar mera dilbar, Hai tu agar mera dilb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105195</th>\n",
       "      <td>@iam_ashima nahi dekhna hai yaar mere KO khhon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105196</th>\n",
       "      <td>tere waaste mera ishq khatmalaana</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105197 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet_text\n",
       "0       @ajuonline #sarcasm Haal chaal thik thak hai-s...\n",
       "1                @I_Am_Hollywood - Charlie Haas? #sarcasm\n",
       "2                        Chetan Bhagat #epicmale #sarcasm\n",
       "3       @Bo_jasim86 LOOL 7atheer affaa 3allaik rgoobti...\n",
       "4       @FENUSF tees2al 3anek el 3afiaa 7abeeb galbe ,...\n",
       "...                                                   ...\n",
       "105192  \"Kaali kaali khaali raaton se Hone lagi hai do...\n",
       "105193  Jao pehle usese jakar puchho jisne meri Jameen...\n",
       "105194  Hai tu agar mera dilbar, Hai tu agar mera dilb...\n",
       "105195  @iam_ashima nahi dekhna hai yaar mere KO khhon...\n",
       "105196                  tere waaste mera ishq khatmalaana\n",
       "\n",
       "[105197 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_pickle('dataset/hinglish_tweet_ids_part2.pkl')\n",
    "print(df2.columns)\n",
    "df2[['tweet_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "581ae9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['tweet_id', 'tweet_text', 'retweet_count', 'favorite_count',\n",
      "       'user_name', 'user_id', 'user_description', 'user_verified',\n",
      "       'user_followers_count', 'user_friends_count', 'user_listed_count',\n",
      "       'user_favourites_count', 'user_statuses_count', 'user_created_at'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@muneebfaruq oho... tention na lo.. paanch saa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@temmujin Buss mujhay bher aik choutee moutee ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pakhtoon K Karname yaad krogey to tmko sharam ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sana_virk wohi na ! zindagi main ek dafa us n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Paap kya punya kya tu bhula de, karma kar phal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7445</th>\n",
       "      <td>@Javedakhtarjadu salaam alecum sir. aapki izaj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7446</th>\n",
       "      <td>Khud Ko Khuda Samjane Wale, Tu Khuda Ka Khouf ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7447</th>\n",
       "      <td>hoti nahi dosti kabhi chehre se, dosti to dil ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7448</th>\n",
       "      <td>Jab Jab Dil Dhadkta Hain Tab Tab Tumhari Yaad ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7449</th>\n",
       "      <td>Jis taraf hum nikal jaayenge / raaste khud ba ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7450 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             tweet_text\n",
       "0     @muneebfaruq oho... tention na lo.. paanch saa...\n",
       "1     @temmujin Buss mujhay bher aik choutee moutee ...\n",
       "2     Pakhtoon K Karname yaad krogey to tmko sharam ...\n",
       "3     @sana_virk wohi na ! zindagi main ek dafa us n...\n",
       "4     Paap kya punya kya tu bhula de, karma kar phal...\n",
       "...                                                 ...\n",
       "7445  @Javedakhtarjadu salaam alecum sir. aapki izaj...\n",
       "7446  Khud Ko Khuda Samjane Wale, Tu Khuda Ka Khouf ...\n",
       "7447  hoti nahi dosti kabhi chehre se, dosti to dil ...\n",
       "7448  Jab Jab Dil Dhadkta Hain Tab Tab Tumhari Yaad ...\n",
       "7449  Jis taraf hum nikal jaayenge / raaste khud ba ...\n",
       "\n",
       "[7450 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = pd.read_pickle('dataset/hinglish_tweet_ids_part3.pkl')\n",
    "print(df3.columns)\n",
    "df3[['tweet_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8df9f156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['tweet_id', 'tweet_text', 'retweet_count', 'favorite_count',\n",
      "       'user_name', 'user_id', 'user_description', 'user_verified',\n",
      "       'user_followers_count', 'user_friends_count', 'user_listed_count',\n",
      "       'user_favourites_count', 'user_statuses_count', 'user_created_at'],\n",
      "      dtype='object')\n",
      "--------------------\n",
      "                 tweet_id                                         tweet_text  \\\n",
      "0              2327708778  @ajuonline #sarcasm Haal chaal thik thak hai-s...   \n",
      "1             15150580702           @I_Am_Hollywood - Charlie Haas? #sarcasm   \n",
      "2             12361902573                   Chetan Bhagat #epicmale #sarcasm   \n",
      "3       40057904656551936  @Bo_jasim86 LOOL 7atheer affaa 3allaik rgoobti...   \n",
      "4       39999071498407936  @FENUSF tees2al 3anek el 3afiaa 7abeeb galbe ,...   \n",
      "...                   ...                                                ...   \n",
      "201233  38780786895233024  @Javedakhtarjadu salaam alecum sir. aapki izaj...   \n",
      "201234  38773918928084992  Khud Ko Khuda Samjane Wale, Tu Khuda Ka Khouf ...   \n",
      "201235  38749874237214720  hoti nahi dosti kabhi chehre se, dosti to dil ...   \n",
      "201236  38743721671524352  Jab Jab Dil Dhadkta Hain Tab Tab Tumhari Yaad ...   \n",
      "201237  38705985702858752  Jis taraf hum nikal jaayenge / raaste khud ba ...   \n",
      "\n",
      "        retweet_count  favorite_count        user_name    user_id  \\\n",
      "0                   0               0            Fagun   14192319   \n",
      "1                   0               0           Johnny   41430949   \n",
      "2                   0               0                R   14297652   \n",
      "3                   0               0    Hamad Al Sane  226071379   \n",
      "4                   0               0    Hamad Al Sane  226071379   \n",
      "...               ...             ...              ...        ...   \n",
      "201233              0               0    vaibhav gupta  177672728   \n",
      "201234              0               0  Shair o Shayari  196104754   \n",
      "201235              0               0        ibiboisms   53337038   \n",
      "201236              0               0  Shair o Shayari  196104754   \n",
      "201237              0               0             خالد   17829306   \n",
      "\n",
      "                                         user_description  user_verified  \\\n",
      "0       Sr QA @ArcanaNetwork; if the engineering team ...          False   \n",
      "1                                                                  False   \n",
      "2                                                      ok          False   \n",
      "3                               Co-owner @TrilogyEventsCo          False   \n",
      "4                               Co-owner @TrilogyEventsCo          False   \n",
      "...                                                   ...            ...   \n",
      "201233                                                             False   \n",
      "201234  Follow to enjoy thousands of urdu and hindi sh...          False   \n",
      "201235                                                             False   \n",
      "201236  Follow to enjoy thousands of urdu and hindi sh...          False   \n",
      "201237  I feel what I feel| Worked as a writer |AMU al...          False   \n",
      "\n",
      "        user_followers_count  user_friends_count  user_listed_count  \\\n",
      "0                       1085                 492                 51   \n",
      "1                       1450                 716                 24   \n",
      "2                       2934                 237                 81   \n",
      "3                        311                 198                  1   \n",
      "4                        311                 198                  1   \n",
      "...                      ...                 ...                ...   \n",
      "201233                    22                  81                  0   \n",
      "201234                  1372                   0                 16   \n",
      "201235                   386                1606                  5   \n",
      "201236                  1372                   0                 16   \n",
      "201237                   826                   0                 13   \n",
      "\n",
      "        user_favourites_count  user_statuses_count           user_created_at  \n",
      "0                        1717                30731 2008-03-21 17:00:38+00:00  \n",
      "1                       21845                36214 2009-05-20 19:37:14+00:00  \n",
      "2                           9               133235 2008-04-03 23:07:46+00:00  \n",
      "3                           5                10023 2010-12-13 06:57:11+00:00  \n",
      "4                           5                10023 2010-12-13 06:57:11+00:00  \n",
      "...                       ...                  ...                       ...  \n",
      "201233                     22                   45 2010-08-12 19:34:53+00:00  \n",
      "201234                      0                51754 2010-09-28 10:27:39+00:00  \n",
      "201235                      0               178229 2009-07-03 08:50:15+00:00  \n",
      "201236                      0                51754 2010-09-28 10:27:39+00:00  \n",
      "201237                  27306                76157 2008-12-03 04:50:35+00:00  \n",
      "\n",
      "[201238 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "df_hinglish = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "print(df_hinglish.columns)\n",
    "print('--------------------')\n",
    "print(df_hinglish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eea4b220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@ajuonline #sarcasm Haal chaal thik thak hai-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@I_Am_Hollywood - Charlie Haas? #sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chetan Bhagat #epicmale #sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Bo_jasim86 LOOL 7atheer affaa 3allaik rgoobti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@FENUSF tees2al 3anek el 3afiaa 7abeeb galbe ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201233</th>\n",
       "      <td>@Javedakhtarjadu salaam alecum sir. aapki izaj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201234</th>\n",
       "      <td>Khud Ko Khuda Samjane Wale, Tu Khuda Ka Khouf ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201235</th>\n",
       "      <td>hoti nahi dosti kabhi chehre se, dosti to dil ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201236</th>\n",
       "      <td>Jab Jab Dil Dhadkta Hain Tab Tab Tumhari Yaad ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201237</th>\n",
       "      <td>Jis taraf hum nikal jaayenge / raaste khud ba ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>201238 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet_text\n",
       "0       @ajuonline #sarcasm Haal chaal thik thak hai-s...\n",
       "1                @I_Am_Hollywood - Charlie Haas? #sarcasm\n",
       "2                        Chetan Bhagat #epicmale #sarcasm\n",
       "3       @Bo_jasim86 LOOL 7atheer affaa 3allaik rgoobti...\n",
       "4       @FENUSF tees2al 3anek el 3afiaa 7abeeb galbe ,...\n",
       "...                                                   ...\n",
       "201233  @Javedakhtarjadu salaam alecum sir. aapki izaj...\n",
       "201234  Khud Ko Khuda Samjane Wale, Tu Khuda Ka Khouf ...\n",
       "201235  hoti nahi dosti kabhi chehre se, dosti to dil ...\n",
       "201236  Jab Jab Dil Dhadkta Hain Tab Tab Tumhari Yaad ...\n",
       "201237  Jis taraf hum nikal jaayenge / raaste khud ba ...\n",
       "\n",
       "[201238 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hinglish_slashed = df_hinglish[['tweet_text']].copy()\n",
    "df_hinglish_slashed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af2f27f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93640\n",
      "                                               tweet_text\n",
      "368     Chitti ai hai ai hai chitthi ai hai = I have m...\n",
      "372     Chitti ai hai ai hai chitthi ai hai = I have m...\n",
      "393                    Kim Kardashian is virgin. #sarcasm\n",
      "394                    Kim Kardashian is virgin. #sarcasm\n",
      "423                    Kim Kardashian is virgin. #sarcasm\n",
      "...                                                   ...\n",
      "200973  http://tinyurl.com/32k296l / LOVE QUOTES / Hic...\n",
      "201054  namskar ek sher aapke kadmo me- \"KHAR CHUBHE K...\n",
      "201171  http://tinyurl.com/4z6sz59 / LOVE QUOTES / Ham...\n",
      "201196  http://tinyurl.com/5vbq3rh / POETRY QUOTES / K...\n",
      "201207  http://tinyurl.com/32k296l / LOVE QUOTES / Hic...\n",
      "\n",
      "[93640 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "#finds the duplicate rows in the unlabeled Hinglish data\n",
    "\n",
    "duplicate_1 = df_hinglish_slashed[df_hinglish_slashed.duplicated(keep='first')]\n",
    "print(len(duplicate_1))\n",
    "print(duplicate_1)\n",
    "#print(type(duplicate_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32b2b01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107598\n",
      "0\n",
      "Index(['tweet_text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#removes duplicates\n",
    "\n",
    "df_hinglish_slashed = df_hinglish_slashed.drop_duplicates(keep='first')\n",
    "print(len(df_hinglish_slashed))\n",
    "\n",
    "duplicate_2 = df_hinglish_slashed[df_hinglish_slashed.duplicated(keep='first')]\n",
    "print(len(duplicate_2))\n",
    "print(df_hinglish_slashed.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f68fbec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94788\n",
      "Index(['tweet_text', 'is_sarcastic'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_labeled = pd.read_pickle('dataset/data_soumyadeep.pkl')\n",
    "df_labeled = df_labeled[['tweet_text', 'is_sarcastic']].copy()\n",
    "print(len(df_labeled))\n",
    "print(df_labeled.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61b87333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2895\n",
      "                                              tweet_text  is_sarcastic\n",
      "354    RT Apna vyauhar hoga aise jal mein kamal rehat...             0\n",
      "630    @fittaymunhapka n osky baad amMi ny kha k kuch...             0\n",
      "686    RT Teen behan ke baad jo aata putra woh trekha...             0\n",
      "809    Ek aadmi pareshan ho kar Ae bhagwan aisi zinda...             1\n",
      "844    RT Teen behan ke baad jo aata putra woh trekha...             0\n",
      "...                                                  ...           ...\n",
      "94716  RT Teen behan ke baad jo aata putra woh trekha...             0\n",
      "94721  Roz badh'taa huun jahaa'n se aage // Phir wahi...             0\n",
      "94747  RT Chhoota vaibhav schooli shiksha shooro ho g...             0\n",
      "94756                                kya hoga is desh ka             0\n",
      "94763  @rjauhari lol  unka bhi mast hoga..koi recessi...             0\n",
      "\n",
      "[2895 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#finds the duplicate rows in the labeled data\n",
    "\n",
    "duplicate_3 = df_labeled[df_labeled.duplicated(subset=['tweet_text'], keep='first')]\n",
    "print(len(duplicate_3))\n",
    "print(duplicate_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34a99267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91893\n",
      "0\n",
      "Index(['tweet_text', 'is_sarcastic'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#removes duplicates from labeled data\n",
    "\n",
    "df_labeled = df_labeled.drop_duplicates(subset=['tweet_text'], keep='first')\n",
    "print(len(df_labeled))\n",
    "\n",
    "duplicate_4 = df_labeled[df_labeled.duplicated(keep='first')]\n",
    "print(len(duplicate_4))\n",
    "print(df_labeled.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38be3af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              tweet_text  is_sarcastic\n",
      "0      @ajuonline #sarcasm Haal chaal thik thak hai-s...             1\n",
      "1               @I_Am_Hollywood - Charlie Haas? #sarcasm             1\n",
      "2                       Chetan Bhagat #epicmale #sarcasm             1\n",
      "3      @Bo_jasim86 LOOL 7atheer affaa 3allaik rgoobti...             1\n",
      "4      @FENUSF tees2al 3anek el 3afiaa 7abeeb galbe ,...             1\n",
      "...                                                  ...           ...\n",
      "26378  Hurt kerny wala khud bhi hurt ho raha hota hy ...             0\n",
      "26379  Zindgi me aage badhna hai to 2 baaten yaad rak...             0\n",
      "26380  @eDeepika  Bus Itna Hi CHahna Mere Baad Merii ...             0\n",
      "26381  18FEB:Jaruri Nahi Aap Her Galti Khud Ker Ke Si...             0\n",
      "26382  18FEB:Jaruri Nahi Aap Her Galti Khud Ker Ke Si...             0\n",
      "\n",
      "[26383 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26383"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finds the common rows between unlabeled Hinglish embedding data and labeled data\n",
    "\n",
    "common_rows = pd.merge(df_hinglish_slashed, df_labeled, how = 'inner', on=['tweet_text'])\n",
    "print(common_rows)\n",
    "len(common_rows)\n",
    "#print(type(common_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "977b9c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81215\n",
      "Index(['tweet_text'], dtype='object')\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7299/3615633645.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_hinglish_slashed.drop(df_hinglish_slashed[common].index, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "#removes all the common rows from unlabeled data\n",
    "\n",
    "common = df_hinglish_slashed['tweet_text'].isin(df_labeled['tweet_text'])\n",
    "df_hinglish_slashed.drop(df_hinglish_slashed[common].index, inplace=True)\n",
    "print(len(df_hinglish_slashed))\n",
    "print(df_hinglish_slashed.columns)\n",
    "\n",
    "common_rows2 = pd.merge(df_hinglish_slashed, df_labeled, how = 'inner', on=['tweet_text'])\n",
    "#print(common_rows2)\n",
    "print(len(common_rows2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fcd05088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3ajeeb make up hayoona #sarcasm'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removes all rows from the Dataframe containing Devanagari script (any non-English word) \n",
    "\n",
    "df_hinglish_slashed = df_hinglish_slashed[df_hinglish_slashed['tweet_text'].map(lambda x: x.isascii())]\n",
    "#filtered_data.iloc[0:5, 1:4]\n",
    "df_hinglish_slashed.iloc[4, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c0aa351",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7299/3460830347.py:6: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  source = source.str.replace(\"\\w*\\d\\w*\",\"\") #removes all words containing numbers\n",
      "/tmp/ipykernel_7299/3460830347.py:10: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  source = source.str.replace(\"[@A-Za-z0-9_]*@[@A-Za-z0-9_]*\",\"\")\n",
      "/tmp/ipykernel_7299/3460830347.py:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  source = source.str.replace(\"[#A-Za-z0-9_]*#[#A-Za-z0-9_]*\",\"\")\n",
      "/tmp/ipykernel_7299/3460830347.py:13: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  source = source.str.replace(\"\\s+|[!$%&()*+, -./:;<=>?\\^_`{|}~]\\s*\",\" \")\n",
      "/tmp/ipykernel_7299/3460830347.py:14: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  source = source.str.replace(\"http\\S+\",\"\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>dary emratiba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>yograj singh  dhoni advised dd management to r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>seedhe log hai ma beta ab  crore ka ghotala ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>ab ham khade ho gaye hain   ab ham aur bhi dar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>make up hayoona</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            tweet_text\n",
       "167                                      dary emratiba\n",
       "194  yograj singh  dhoni advised dd management to r...\n",
       "200  seedhe log hai ma beta ab  crore ka ghotala ho...\n",
       "309  ab ham khade ho gaye hain   ab ham aur bhi dar...\n",
       "470                                    make up hayoona"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removes mentions, '#', emojis, punctuations, special chars, URLs, words containing numbers, spaces from beginning and end of tweets\n",
    "\n",
    "def data_preprocessing(source):\n",
    "    \n",
    "    source = source.str.lower() #lower case chars\n",
    "    source = source.str.replace(\"\\w*\\d\\w*\",\"\") #removes all words containing numbers\n",
    "    #source = source.str.replace(\"#\", \"\")\n",
    "    #source = source.str.replace(\"@\", \"\")\n",
    "    #source = source.str.replace(\"@[A-Za-z0-9_]+\",\"\")\n",
    "    source = source.str.replace(\"[@A-Za-z0-9_]*@[@A-Za-z0-9_]*\",\"\")\n",
    "    #source = source.str.replace(\"#[A-Za-z0-9_]+\",\"\")\n",
    "    source = source.str.replace(\"[#A-Za-z0-9_]*#[#A-Za-z0-9_]*\",\"\")\n",
    "    source = source.str.replace(\"\\s+|[!$%&()*+, -./:;<=>?\\^_`{|}~]\\s*\",\" \")\n",
    "    source = source.str.replace(\"http\\S+\",\"\")\n",
    "    \n",
    "    return source\n",
    "\n",
    "df_hinglish_slashed['tweet_text'] = data_preprocessing(df_hinglish_slashed ['tweet_text'])\n",
    "df_hinglish_slashed['tweet_text'] = df_hinglish_slashed['tweet_text'].apply(lambda s: emoji.replace_emoji(s,\"\"))\n",
    "df_hinglish_slashed['tweet_text'] = df_hinglish_slashed['tweet_text'].str.strip()\n",
    "df_hinglish_slashed[['tweet_text']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acfec342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'make up hayoona'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hinglish_slashed.iloc[4, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3784c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78776\n"
     ]
    }
   ],
   "source": [
    "#finds the unique words in the unlabeled data\n",
    "\n",
    "unique_words = set()\n",
    "df_hinglish_slashed['tweet_text'].str.lower().str.split().apply(unique_words.update)\n",
    "unique_words = list(unique_words)\n",
    "#print(unique_words)\n",
    "#print('---------------')\n",
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1550bfb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             tweet_text\n",
      "167                                    [dary, emratiba]\n",
      "194   [yograj, singh, dhoni, advised, dd, management...\n",
      "200   [seedhe, log, hai, ma, beta, ab, crore, ka, gh...\n",
      "309   [ab, ham, khade, ho, gaye, hain, ab, ham, aur,...\n",
      "470                                 [make, up, hayoona]\n",
      "571   [cut, marna, koi, umar, se, seekhay, nahi, see...\n",
      "901   [u, r, aapne, chaata, nahi, ki, tarah, pehle, ...\n",
      "1073         [bhai, tu, ek, vaio, mini, laptop, le, le]\n",
      "1273  [aanaayi, janichirunengil, kashtapettu, cash, ...\n",
      "1377  [mere, bulane, se, koi, aata, hi, nhi, sab, bo...\n",
      "73911\n"
     ]
    }
   ],
   "source": [
    "#generates tokenized data for the preprocessed data\n",
    "\n",
    "#nltk.download('punkt')\n",
    "df_hinglish_slashed['tweet_text'] = df_hinglish_slashed['tweet_text'].apply(nltk.word_tokenize)\n",
    "print(df_hinglish_slashed.head(10))\n",
    "print(len(df_hinglish_slashed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7680343a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             tweet_text\n",
      "167                                    [dary, emratiba]\n",
      "194   [yograj, singh, dhoni, advised, dd, management...\n",
      "200   [seedhe, log, hai, ma, beta, ab, crore, ka, gh...\n",
      "309   [ab, ham, khade, ho, gaye, hain, ab, ham, aur,...\n",
      "470                                 [make, up, hayoona]\n",
      "571   [cut, marna, koi, umar, se, seekhay, nahi, see...\n",
      "901   [u, r, aapne, chaata, nahi, ki, tarah, pehle, ...\n",
      "1073         [bhai, tu, ek, vaio, mini, laptop, le, le]\n",
      "1273  [aanaayi, janichirunengil, kashtapettu, cash, ...\n",
      "1377  [mere, bulane, se, koi, aata, hi, nhi, sab, bo...\n",
      "73910\n"
     ]
    }
   ],
   "source": [
    "#removes all the rows with empty lists\n",
    "\n",
    "df_hinglish_slashed=df_hinglish_slashed[df_hinglish_slashed['tweet_text'].map(lambda d: len(d)) > 0]\n",
    "print(df_hinglish_slashed.head(10))\n",
    "print(len(df_hinglish_slashed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f205d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding size---> 300\n"
     ]
    }
   ],
   "source": [
    "#model = Word2Vec(df_hing_eng['tweet_text'], vector_size=300, window=10, hs=0, negative = 1)\n",
    "model = FastText(df_hinglish_slashed['tweet_text'], vector_size=300, window=10, hs=0, negative = 1)\n",
    "embedding_size = model.wv.vectors.shape[1]\n",
    "print(\"embedding size--->\", embedding_size)\n",
    "vocab = model.wv.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f94e2264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!!!\n"
     ]
    }
   ],
   "source": [
    "#saves the FastText model\n",
    "\n",
    "model.save(\"hing_fasttext.model\")\n",
    "print(\"Model saved!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f14aa9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=15582, vector_size=300, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = FastText.load(\"hing_fasttext.model\")\n",
    "print(embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b362b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          tweet_text  is_sarcastic\n",
      "0  LOL !  BC badaa gaandu hai to ! so jaa lodu ! ...             1\n",
      "1  @OFFICIAL_FC_PAK apni sakh khud sambhalen aur ...             0\n",
      "2  Ek aadmi pareshan ho kar Ae bhagwan aisi zinda...             1\n",
      "3  Lol RT @ThePrachi: @shrikhande Who can sing a ...             1\n",
      "4  @idrinkblueblood tu ol's kar rahi ho kya? Kya ...             0\n",
      "91893\n"
     ]
    }
   ],
   "source": [
    "print(df_labeled.head())\n",
    "print(len(df_labeled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "743c6bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7299/3018622983.py:6: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  source = source.str.replace(\"\\w*\\d\\w*\",\"\") #removes all words containing numbers\n",
      "/tmp/ipykernel_7299/3018622983.py:10: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  source = source.str.replace(\"[@A-Za-z0-9_]*@[@A-Za-z0-9_]*\",\"\")\n",
      "/tmp/ipykernel_7299/3018622983.py:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  source = source.str.replace(\"[#A-Za-z0-9_]*#[#A-Za-z0-9_]*\",\"\")\n",
      "/tmp/ipykernel_7299/3018622983.py:13: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  source = source.str.replace(\"\\s+|[!$%&()*+, -./:;<=>?\\^_`{|}~]\\s*\",\" \")\n",
      "/tmp/ipykernel_7299/3018622983.py:14: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  source = source.str.replace(\"http\\S+\",\"\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lol  bc badaa gaandu hai to  so jaa lodu  tere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apni sakh khud sambhalen aur kisi ka saath na ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ek aadmi pareshan ho kar ae bhagwan aisi zinda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lol rt  who can sing a group song alone raavan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tu ol s kar rahi ho kya kya hoga tere kaam sir...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text\n",
       "0  lol  bc badaa gaandu hai to  so jaa lodu  tere...\n",
       "1  apni sakh khud sambhalen aur kisi ka saath na ...\n",
       "2  ek aadmi pareshan ho kar ae bhagwan aisi zinda...\n",
       "3  lol rt  who can sing a group song alone raavan...\n",
       "4  tu ol s kar rahi ho kya kya hoga tere kaam sir..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removes mentions, '#', emojis, punctuations, special chars, URLs, words containing numbers, spaces from beginning and end of tweets\n",
    "\n",
    "def data_preprocessing(source):\n",
    "    \n",
    "    source = source.str.lower() #lower case chars\n",
    "    source = source.str.replace(\"\\w*\\d\\w*\",\"\") #removes all words containing numbers\n",
    "    #source = source.str.replace(\"#\", \"\")\n",
    "    #source = source.str.replace(\"@\", \"\")\n",
    "    #source = source.str.replace(\"@[A-Za-z0-9_]+\",\"\")\n",
    "    source = source.str.replace(\"[@A-Za-z0-9_]*@[@A-Za-z0-9_]*\",\"\")\n",
    "    #source = source.str.replace(\"#[A-Za-z0-9_]+\",\"\")\n",
    "    source = source.str.replace(\"[#A-Za-z0-9_]*#[#A-Za-z0-9_]*\",\"\")\n",
    "    source = source.str.replace(\"\\s+|[!$%&()*+, -./:;<=>?\\^_`{|}~]\\s*\",\" \")\n",
    "    source = source.str.replace(\"http\\S+\",\"\")\n",
    "    \n",
    "    return source\n",
    "\n",
    "df_labeled['tweet_text'] = data_preprocessing(df_labeled['tweet_text'])\n",
    "df_labeled['tweet_text'] = df_labeled['tweet_text'].apply(lambda s: emoji.replace_emoji(s,\"\"))\n",
    "df_labeled['tweet_text'] = df_labeled['tweet_text'].str.strip()\n",
    "df_labeled[['tweet_text']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e826d40e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lol  bc badaa gaandu hai to  so jaa lodu  tere...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apni sakh khud sambhalen aur kisi ka saath na ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ek aadmi pareshan ho kar ae bhagwan aisi zinda...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lol rt  who can sing a group song alone raavan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tu ol s kar rahi ho kya kya hoga tere kaam sir...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  is_sarcastic\n",
       "0  lol  bc badaa gaandu hai to  so jaa lodu  tere...             1\n",
       "1  apni sakh khud sambhalen aur kisi ka saath na ...             0\n",
       "2  ek aadmi pareshan ho kar ae bhagwan aisi zinda...             1\n",
       "3  lol rt  who can sing a group song alone raavan...             1\n",
       "4  tu ol s kar rahi ho kya kya hoga tere kaam sir...             0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "149e2bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93958\n"
     ]
    }
   ],
   "source": [
    "#finds the unique words in the labeled data\n",
    "\n",
    "unique_words = set()\n",
    "df_labeled['tweet_text'].str.lower().str.split().apply(unique_words.update)\n",
    "unique_words = list(unique_words)\n",
    "#print(unique_words)\n",
    "#print('---------------')\n",
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9d66ac76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[lol, bc, badaa, gaandu, hai, to, so, jaa, lod...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[apni, sakh, khud, sambhalen, aur, kisi, ka, s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ek, aadmi, pareshan, ho, kar, ae, bhagwan, ai...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[lol, rt, who, can, sing, a, group, song, alon...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[tu, ol, s, kar, rahi, ho, kya, kya, hoga, ter...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  is_sarcastic\n",
       "0  [lol, bc, badaa, gaandu, hai, to, so, jaa, lod...             1\n",
       "1  [apni, sakh, khud, sambhalen, aur, kisi, ka, s...             0\n",
       "2  [ek, aadmi, pareshan, ho, kar, ae, bhagwan, ai...             1\n",
       "3  [lol, rt, who, can, sing, a, group, song, alon...             1\n",
       "4  [tu, ol, s, kar, rahi, ho, kya, kya, hoga, ter...             0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generates tokenized data \n",
    "\n",
    "#nltk.download('punkt')\n",
    "df_labeled['tweet_text'] = df_labeled['tweet_text'].apply(nltk.word_tokenize)\n",
    "df_labeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "720a304b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91893\n",
      "                                          tweet_text  is_sarcastic\n",
      "0  [lol, bc, badaa, gaandu, hai, to, so, jaa, lod...             1\n",
      "1  [apni, sakh, khud, sambhalen, aur, kisi, ka, s...             0\n",
      "2  [ek, aadmi, pareshan, ho, kar, ae, bhagwan, ai...             1\n",
      "3  [lol, rt, who, can, sing, a, group, song, alon...             1\n",
      "4  [tu, ol, s, kar, rahi, ho, kya, kya, hoga, ter...             0\n",
      "5      [yaar, ye, log, tu, serious, ho, gaye, t, co]             0\n",
      "6  [abey, tumhare, paas, kaam, nahi, hai, jo, twi...             0\n",
      "7  [ani, tauko, dukhyo, vanera, jaane, ta, ho, ni...             0\n",
      "8  [rt, agar, tumne, rt, nahi, kiya, to, mujhse, ...             0\n",
      "9  [dil, ki, dhadkan, sooni, sooni, saansain, bhi...             0\n",
      "91892\n"
     ]
    }
   ],
   "source": [
    "#removes all the rows with empty lists\n",
    "\n",
    "print(len(df_labeled))\n",
    "df_labeled = df_labeled[df_labeled['tweet_text'].map(lambda d: len(d)) > 0]\n",
    "print(df_labeled.head(10))\n",
    "print(len(df_labeled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "74589ce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[lol, bc, badaa, gaandu, hai, to, so, jaa, lod...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[apni, sakh, khud, sambhalen, aur, kisi, ka, s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ek, aadmi, pareshan, ho, kar, ae, bhagwan, ai...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[lol, rt, who, can, sing, a, group, song, alon...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[tu, ol, s, kar, rahi, ho, kya, kya, hoga, ter...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  is_sarcastic\n",
       "0  [lol, bc, badaa, gaandu, hai, to, so, jaa, lod...             1\n",
       "1  [apni, sakh, khud, sambhalen, aur, kisi, ka, s...             0\n",
       "2  [ek, aadmi, pareshan, ho, kar, ae, bhagwan, ai...             1\n",
       "3  [lol, rt, who, can, sing, a, group, song, alon...             1\n",
       "4  [tu, ol, s, kar, rahi, ho, kya, kya, hoga, ter...             0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "561d3da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>[dary, emratiba]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>[yograj, singh, dhoni, advised, dd, management...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>[seedhe, log, hai, ma, beta, ab, crore, ka, gh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>[ab, ham, khade, ho, gaye, hain, ab, ham, aur,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>[make, up, hayoona]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            tweet_text\n",
       "167                                   [dary, emratiba]\n",
       "194  [yograj, singh, dhoni, advised, dd, management...\n",
       "200  [seedhe, log, hai, ma, beta, ab, crore, ka, gh...\n",
       "309  [ab, ham, khade, ho, gaye, hain, ab, ham, aur,...\n",
       "470                                [make, up, hayoona]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hinglish_slashed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5069f5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "x_train_shape---> (64324,)\n",
      "y_train_shape---> (64324,)\n",
      "x_test_shape---> (27568,)\n",
      "y_test_shape---> (27568,)\n",
      "<class 'pandas.core.series.Series'>\n",
      "contents of first label---> 1\n"
     ]
    }
   ],
   "source": [
    "#train test split of labeled data\n",
    "\n",
    "check_nan = df_labeled['is_sarcastic'].isnull().values.any()\n",
    "print(check_nan)\n",
    "check_na = df_labeled['is_sarcastic'].isna().values.any()\n",
    "print(check_na)\n",
    "check_nan_ = df_labeled['tweet_text'].isnull().values.any()\n",
    "print(check_nan_)\n",
    "check_na_ = df_labeled['tweet_text'].isna().values.any()\n",
    "print(check_na_)\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(df_labeled['tweet_text'], df_labeled['is_sarcastic'], test_size=0.30, random_state=42)\n",
    "#x_train,x_test,y_train,y_test=train_test_split(padded_tweets, sarcasm_labels, test_size=0.20, random_state=42)\n",
    "print(\"x_train_shape--->\", x_train.shape)\n",
    "print(\"y_train_shape--->\", y_train.shape)\n",
    "print(\"x_test_shape--->\", x_test.shape)\n",
    "print(\"y_test_shape--->\", y_test.shape)\n",
    "print(type(x_train))\n",
    "print(\"contents of first label--->\", y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9b634532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39384    [kisi, siyaasat, daan, ki, taraah, tumhen, haa...\n",
      "70407    [agar, bharat, ko, hai, mahaan, banna, toh, bh...\n",
      "12711    [atal, bihari, vajpayee, and, nawaz, sharif, s...\n",
      "20467    [bara, shok, tha, unhain, mera, ayeshana, daik...\n",
      "15898    [wo, aap, ne, sunna, hee, hoga, goray, kaam, k...\n",
      "                               ...                        \n",
      "6316     [oye, yaar, tera, dimag, kharab, hai, kya, abh...\n",
      "56186    [mulke, halate, jung, mein, hai, mujhe, koi, s...\n",
      "79025    [mastar, bandya, mashi, aani, daasa, madhe, ka...\n",
      "865      [ek, kunwari, larki, pehli, baar, ek, larke, s...\n",
      "16017    [always, joking, jo, chij, kbhi, sudhar, nhi, ...\n",
      "Name: tweet_text, Length: 64324, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d7176a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    32482\n",
      "1    31842\n",
      "Name: is_sarcastic, dtype: int64\n",
      "--------------------------\n",
      "0    13962\n",
      "1    13606\n",
      "Name: is_sarcastic, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_train.value_counts())\n",
    "print('--------------------------')\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "64295d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length---> 84\n",
      "Mean length---> 16.626049375038864\n",
      "Standard Deviation of length---> 9.42481679865071\n",
      "Maximum length of the sequence---> 26\n"
     ]
    }
   ],
   "source": [
    "length = []\n",
    "for row in x_train:\n",
    "    #print(row)\n",
    "    #print(len(row))\n",
    "    #count = count+1\n",
    "    #if len(row)>maxi:\n",
    "    #print(len(row))\n",
    "    length.append(len(row))\n",
    "        \n",
    "#print(length)\n",
    "print(\"Max length--->\", max(length))\n",
    "print(\"Mean length--->\", np.mean(length))\n",
    "print(\"Standard Deviation of length--->\", np.std(length))\n",
    "\n",
    "#maxi = max(length)\n",
    "maxi = np.round(np.mean(length) + np.std(length)).astype(int)\n",
    "print(\"Maximum length of the sequence--->\", maxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "492d1ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['kisi', 'siyaasat', 'daan', 'ki', 'taraah', 'tumhen', 'haasil', 'karne', 'k', 'liye', 'roz', 'naye', 'jalse', 'karta', 'hoon', 'apne', 'dil', 'men'], ['agar', 'bharat', 'ko', 'hai', 'mahaan', 'banna', 'toh', 'bhrasht', 'netaao', 'ko', 'hoga', 'hatna', 'aur', 'bhrashtachaar', 'ko', 'hoga', 'mitaana', 'yeh', 'kisi', 'ek', 'seh', 'na', 'hoga', 'pureh', 'jansamuday', 'ko', 'hoga', 'saath', 'nibhaana', 'happy', 'independence', 'day'], ['atal', 'bihari', 'vajpayee', 'and', 'nawaz', 'sharif', 'share', 'the', 'same', 'date', 'of', 'birth'], ['bara', 'shok', 'tha', 'unhain', 'mera', 'ayeshana', 'daiknay', 'ka', 'naddan', 'jab', 'daiki', 'meri', 'garabi', 'tu', 'rasta', 'badal', 'liya'], ['wo', 'aap', 'ne', 'sunna', 'hee', 'hoga', 'goray', 'kaam', 'ko', 'dimaag', 'mei', 'rakhtay', 'hain', 'aur', 'hehe', 'magar', 'pakistani', 'uss', 'ko', 'dimag', 'mei', 'aur', 'kaam', 'ko']]\n"
     ]
    }
   ],
   "source": [
    "total_docs=[]\n",
    "total_docs.extend(x_train)\n",
    "total_docs.extend(x_test)\n",
    "print(total_docs[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0f4f610a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing input data...\n"
     ]
    }
   ],
   "source": [
    "print(\"tokenizing input data...\")\n",
    "MAX_NB_WORDS = 200000\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(total_docs)\n",
    "#tokenizer.fit_on_texts(x_train)\n",
    "word_seq_train = tokenizer.texts_to_sequences(x_train)\n",
    "word_seq_test = tokenizer.texts_to_sequences(x_test)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c1cbad4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64324\n",
      "[58, 17091, 5027, 2, 25830, 1972, 4437, 122, 11, 42, 391, 1217, 9317, 149, 163, 87, 13, 380]\n",
      "27568\n",
      "[695, 4, 113, 623, 14, 172, 16, 76773, 5277, 7, 113, 623]\n",
      "dictionary size:  93854\n"
     ]
    }
   ],
   "source": [
    "print(len(word_seq_train))\n",
    "print(word_seq_train[0])\n",
    "print(len(word_seq_test))\n",
    "print(word_seq_test[0])\n",
    "print(\"dictionary size: \", len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "53eb6dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   58 17091  5027     2 25830  1972  4437   122    11    42   391  1217\n",
      "  9317   149   163    87    13   380     0     0     0     0     0     0\n",
      "     0     0]\n",
      "(64324, 26)\n",
      "(27568, 26)\n"
     ]
    }
   ],
   "source": [
    "#pads to the length of the longest sequence\n",
    "\n",
    "word_seq_train = pad_sequences(word_seq_train, maxlen=maxi, padding='post')\n",
    "word_seq_test = pad_sequences(word_seq_test, maxlen=maxi, padding='post')\n",
    "print(word_seq_train[0])\n",
    "print(word_seq_train.shape)\n",
    "print(word_seq_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "12838d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing embedding matrix...\n",
      "number of null word embeddings: 0\n",
      "(93855, 300)\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "#builds the embedding matrix\n",
    "\n",
    "print('preparing embedding matrix...')\n",
    "embed_dim = embedding_size\n",
    "words_not_found = []\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.random.rand(nb_words+1, embed_dim)\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    #print(word)\n",
    "\n",
    "    if embeddings_index.wv.__contains__(word): \n",
    "\n",
    "        embedding_vector = embeddings_index.wv[word]\n",
    "    \n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "#print(embedding_matrix)\n",
    "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "print(embedding_matrix.shape)\n",
    "print(len(embedding_matrix[34]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "46bb1933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(words_not_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "783ac71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
    "callbacks_list = [early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3faada4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attention BiLSTM\n",
    "\n",
    "from keras import backend as K\n",
    "#from keras.engine.topology import Layer\n",
    "from tensorflow.keras.layers import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight( shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "        #print(self.name)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8cd298b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-01 13:00:42.729658: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-05-01 13:00:42.759112: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-05-01 13:00:42.861833: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (user-PC): /proc/driver/nvidia/version does not exist\n",
      "2023-05-01 13:00:43.155370: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-01 13:00:46.635238: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 112626000 exceeds 10% of free system memory.\n",
      "2023-05-01 13:00:48.712207: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 112626000 exceeds 10% of free system memory.\n",
      "2023-05-01 13:00:49.593863: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 112626000 exceeds 10% of free system memory.\n",
      "2023-05-01 13:00:51.279792: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 112626000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 26)]              0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 26, 300)           28156500  \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 26, 300)          541200    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " attention (Attention)       (None, 300)               326       \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               77056     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,775,339\n",
      "Trainable params: 618,839\n",
      "Non-trainable params: 28,156,500\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "lstm_out1 = 150\n",
    "#bilstm_out1 = 96\n",
    "#lstm_out2 = 32\n",
    "#dropout = 0.2\n",
    "#recurrent_dropout = 0.2\n",
    "\n",
    "inp = Input(shape=(maxi, ))\n",
    "x = Embedding(nb_words+1, embed_dim, weights=[embedding_matrix], trainable=False)(inp)\n",
    "x = Bidirectional(LSTM(lstm_out1, return_sequences=True, dropout=0.20, recurrent_dropout=0.20))(x)\n",
    "x = Attention(maxi)(x)\n",
    "x = Dense(256, activation=\"relu\")(x)\n",
    "#x = Dropout(0.25)(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a9188b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "905/905 [==============================] - 289s 301ms/step - loss: 0.3935 - accuracy: 0.8271 - val_loss: 0.2794 - val_accuracy: 0.9027\n",
      "Epoch 2/20\n",
      "905/905 [==============================] - 292s 323ms/step - loss: 0.2307 - accuracy: 0.9221 - val_loss: 0.2042 - val_accuracy: 0.9411\n",
      "Epoch 3/20\n",
      "905/905 [==============================] - 268s 296ms/step - loss: 0.1774 - accuracy: 0.9473 - val_loss: 0.1723 - val_accuracy: 0.9503\n",
      "Epoch 4/20\n",
      "905/905 [==============================] - 323s 357ms/step - loss: 0.1624 - accuracy: 0.9511 - val_loss: 0.1729 - val_accuracy: 0.9495\n",
      "Epoch 5/20\n",
      "905/905 [==============================] - 371s 410ms/step - loss: 0.1544 - accuracy: 0.9531 - val_loss: 0.1664 - val_accuracy: 0.9513\n",
      "Epoch 6/20\n",
      "905/905 [==============================] - 377s 417ms/step - loss: 0.1479 - accuracy: 0.9539 - val_loss: 0.1753 - val_accuracy: 0.9518\n",
      "Epoch 7/20\n",
      "905/905 [==============================] - 316s 349ms/step - loss: 0.1397 - accuracy: 0.9560 - val_loss: 0.1687 - val_accuracy: 0.9517\n",
      "Epoch 7: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9254022370>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trains attention BiLSTM model \n",
    "\n",
    "model.fit(word_seq_train, y_train, batch_size=64, epochs=20, validation_split=0.1, shuffle=True, callbacks = callbacks_list)\n",
    "#model.save('att_bilstm.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "41f56b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "862/862 [==============================] - 54s 62ms/step\n",
      "Accuracy is---> 95.08125362739408\n"
     ]
    }
   ],
   "source": [
    "#model = load_model('att_bilstm.h5')\n",
    "y_pred_proba = model.predict(word_seq_test)\n",
    "#print(y_pred_proba)\n",
    "#print('---------------')\n",
    "\n",
    "y_pred_class = np.round(y_pred_proba)\n",
    "y_pred_class = y_pred_class.squeeze() #makes y_pred_class 1 dim\n",
    "'''print(y_pred_class)\n",
    "print(type(y_pred_class))\n",
    "print(len(y_pred_class))\n",
    "print(len(y_test))\n",
    "print('---------------')'''\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred_class) * 100\n",
    "print(\"Accuracy is--->\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6076472c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f684b94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-01 13:40:50.707322: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 112626000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 26, 300)           28156500  \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 150)               270600    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                9664      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,436,829\n",
      "Trainable params: 280,329\n",
      "Non-trainable params: 28,156,500\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#this one--->LSTM\n",
    "\n",
    "lstm_out1 = 150\n",
    "'''bilstm_out1 = 96\n",
    "lstm_out2 = 32\n",
    "dropout = 0.2\n",
    "recurrent_dropout = 0.2'''\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_words+1, embed_dim, weights=[embedding_matrix], input_length=maxi, trainable=False))\n",
    "model.add(LSTM(lstm_out1, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "24c7600a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "905/905 [==============================] - 143s 153ms/step - loss: 0.5032 - accuracy: 0.7462 - val_loss: 0.3462 - val_accuracy: 0.8693\n",
      "Epoch 2/20\n",
      "905/905 [==============================] - 137s 151ms/step - loss: 0.3110 - accuracy: 0.8875 - val_loss: 0.2773 - val_accuracy: 0.9032\n",
      "Epoch 3/20\n",
      "905/905 [==============================] - 137s 152ms/step - loss: 0.2700 - accuracy: 0.9036 - val_loss: 0.2598 - val_accuracy: 0.9084\n",
      "Epoch 4/20\n",
      "905/905 [==============================] - 138s 152ms/step - loss: 0.2475 - accuracy: 0.9122 - val_loss: 0.2335 - val_accuracy: 0.9199\n",
      "Epoch 5/20\n",
      "905/905 [==============================] - 139s 154ms/step - loss: 0.2155 - accuracy: 0.9286 - val_loss: 0.2258 - val_accuracy: 0.9305\n",
      "Epoch 6/20\n",
      "905/905 [==============================] - 132s 146ms/step - loss: 0.1866 - accuracy: 0.9419 - val_loss: 0.1857 - val_accuracy: 0.9448\n",
      "Epoch 7/20\n",
      "905/905 [==============================] - 132s 146ms/step - loss: 0.1710 - accuracy: 0.9481 - val_loss: 0.1771 - val_accuracy: 0.9498\n",
      "Epoch 8/20\n",
      "905/905 [==============================] - 149s 164ms/step - loss: 0.1622 - accuracy: 0.9503 - val_loss: 0.1734 - val_accuracy: 0.9484\n",
      "Epoch 9/20\n",
      "905/905 [==============================] - 148s 163ms/step - loss: 0.1557 - accuracy: 0.9525 - val_loss: 0.1728 - val_accuracy: 0.9479\n",
      "Epoch 10/20\n",
      "905/905 [==============================] - 152s 168ms/step - loss: 0.1478 - accuracy: 0.9538 - val_loss: 0.1750 - val_accuracy: 0.9507\n",
      "Epoch 11/20\n",
      "905/905 [==============================] - 152s 168ms/step - loss: 0.1430 - accuracy: 0.9548 - val_loss: 0.1736 - val_accuracy: 0.9503\n",
      "Epoch 12/20\n",
      "905/905 [==============================] - 156s 172ms/step - loss: 0.1363 - accuracy: 0.9564 - val_loss: 0.1732 - val_accuracy: 0.9503\n",
      "Epoch 12: early stopping\n"
     ]
    }
   ],
   "source": [
    "#trains the LSTM model\n",
    "\n",
    "model.fit(word_seq_train, y_train, batch_size=64, epochs=20, validation_split=0.1, shuffle=True, callbacks = callbacks_list)\n",
    "model.save('lstm.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5fe33610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "862/862 [==============================] - 26s 29ms/step\n",
      "Accuracy is---> 94.80919907138711\n"
     ]
    }
   ],
   "source": [
    "#model = load_model('att_bilstm.h5')\n",
    "y_pred_proba = model.predict(word_seq_test)\n",
    "#print(y_pred_proba)\n",
    "#print('---------------')\n",
    "\n",
    "y_pred_class = np.round(y_pred_proba)\n",
    "y_pred_class = y_pred_class.squeeze() #makes y_pred_class 1 dim\n",
    "'''print(y_pred_class)\n",
    "print(type(y_pred_class))\n",
    "print(len(y_pred_class))\n",
    "print(len(y_test))\n",
    "print('---------------')'''\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred_class) * 100\n",
    "print(\"Accuracy is--->\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e033e59a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "82c27335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 26)]              0         \n",
      "                                                                 \n",
      " embedding_2 (Embedding)     (None, 26, 300)           28156500  \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 26, 300)          541200    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 26, 10)           3010      \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 260)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 64)                16704     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,717,479\n",
      "Trainable params: 560,979\n",
      "Non-trainable params: 28,156,500\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# BiLSTM\n",
    "\n",
    "lstm_out1 = 150\n",
    "bilstm_out1 = 96\n",
    "lstm_out2 = 32\n",
    "dropout = 0.2\n",
    "recurrent_dropout = 0.2\n",
    "\n",
    "# main model\n",
    "inputt = Input(shape=(maxi,))\n",
    "model = Embedding(nb_words+1, embed_dim, weights=[embedding_matrix],input_length=maxi,trainable = False)(inputt)\n",
    "model =  Bidirectional (LSTM (lstm_out1, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode='concat')(model)\n",
    "model = TimeDistributed(Dense(10,activation='relu'))(model)\n",
    "model = Flatten()(model)\n",
    "model = Dense(64,activation='relu')(model)\n",
    "output = Dense(1,activation='sigmoid')(model)\n",
    "model = Model(inputt,output)\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c9757263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "905/905 [==============================] - 278s 297ms/step - loss: 0.4614 - accuracy: 0.7790 - val_loss: 0.3079 - val_accuracy: 0.8955\n",
      "Epoch 2/20\n",
      "905/905 [==============================] - 288s 319ms/step - loss: 0.2804 - accuracy: 0.9014 - val_loss: 0.2836 - val_accuracy: 0.9053\n",
      "Epoch 3/20\n",
      "905/905 [==============================] - 268s 296ms/step - loss: 0.2443 - accuracy: 0.9125 - val_loss: 0.2216 - val_accuracy: 0.9252\n",
      "Epoch 4/20\n",
      "905/905 [==============================] - 303s 335ms/step - loss: 0.1792 - accuracy: 0.9444 - val_loss: 0.1767 - val_accuracy: 0.9471\n",
      "Epoch 5/20\n",
      "905/905 [==============================] - 283s 313ms/step - loss: 0.1548 - accuracy: 0.9515 - val_loss: 0.1659 - val_accuracy: 0.9485\n",
      "Epoch 6/20\n",
      "905/905 [==============================] - 283s 313ms/step - loss: 0.1417 - accuracy: 0.9548 - val_loss: 0.1602 - val_accuracy: 0.9510\n",
      "Epoch 7/20\n",
      "905/905 [==============================] - 283s 313ms/step - loss: 0.1290 - accuracy: 0.9572 - val_loss: 0.1689 - val_accuracy: 0.9476\n",
      "Epoch 8/20\n",
      "905/905 [==============================] - 283s 313ms/step - loss: 0.1156 - accuracy: 0.9611 - val_loss: 0.1683 - val_accuracy: 0.9496\n",
      "Epoch 9/20\n",
      "905/905 [==============================] - 283s 312ms/step - loss: 0.1029 - accuracy: 0.9643 - val_loss: 0.1715 - val_accuracy: 0.9520\n",
      "Epoch 9: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.fit(word_seq_train, y_train, batch_size=64, epochs=20, validation_split=0.1, shuffle=True, callbacks = callbacks_list)\n",
    "model.save('bilstm.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "12439977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "862/862 [==============================] - 51s 58ms/step\n",
      "Accuracy is---> 95.00145095763204\n"
     ]
    }
   ],
   "source": [
    "#model = load_model('att_bilstm.h5')\n",
    "y_pred_proba = model.predict(word_seq_test)\n",
    "#print(y_pred_proba)\n",
    "#print('---------------')\n",
    "\n",
    "y_pred_class = np.round(y_pred_proba)\n",
    "y_pred_class = y_pred_class.squeeze() #makes y_pred_class 1 dim\n",
    "'''print(y_pred_class)\n",
    "print(type(y_pred_class))\n",
    "print(len(y_pred_class))\n",
    "print(len(y_test))\n",
    "print('---------------')'''\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred_class) * 100\n",
    "print(\"Accuracy is--->\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c588f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "648f0de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training CNN ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 26, 300)           28156500  \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 26, 200)           420200    \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 13, 200)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 13, 200)           280200    \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 6, 200)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 200)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 200)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 128)               25728     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,890,949\n",
      "Trainable params: 734,449\n",
      "Non-trainable params: 28,156,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#CNN1\n",
    "\n",
    "#model parameters\n",
    "\n",
    "num_kernels=200\n",
    "stride=1\n",
    "embed_dim = 300\n",
    "weight_decay = 1e-4\n",
    "\n",
    "#parallel layers \n",
    "kernel_size=7\n",
    "kernel_size_p1=3\n",
    "kernel_size_p2=6\n",
    "kernel_size_p3=9\n",
    "kernel_size_p4=12\n",
    "\n",
    "#CNN architecture\n",
    "print(\"training CNN ...\")\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_words+1, embed_dim, weights=[embedding_matrix], input_length=maxi, trainable=False))\n",
    "model.add(Conv1D(num_kernels, kernel_size, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Conv1D(num_kernels, kernel_size, activation='relu', padding='same'))\n",
    "#model.add(GlobalMaxPooling1D())\n",
    "model.add(MaxPooling1D(2))\n",
    "#model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "#model.add(Dense(num_classes, activation='sigmoid'))  #multi-label (k-hot encoding)\n",
    "model.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9238b22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "905/905 [==============================] - 100s 110ms/step - loss: 0.2504 - accuracy: 0.9029 - val_loss: 0.3498 - val_accuracy: 0.8632\n",
      "Epoch 2/20\n",
      "905/905 [==============================] - 99s 110ms/step - loss: 0.2349 - accuracy: 0.9093 - val_loss: 0.3661 - val_accuracy: 0.8579\n",
      "Epoch 3/20\n",
      "905/905 [==============================] - 100s 110ms/step - loss: 0.2247 - accuracy: 0.9127 - val_loss: 0.3830 - val_accuracy: 0.8609\n",
      "Epoch 4/20\n",
      "905/905 [==============================] - 100s 110ms/step - loss: 0.2120 - accuracy: 0.9175 - val_loss: 0.3892 - val_accuracy: 0.8601\n",
      "Epoch 5/20\n",
      "905/905 [==============================] - 100s 110ms/step - loss: 0.2035 - accuracy: 0.9221 - val_loss: 0.3766 - val_accuracy: 0.8637\n",
      "Epoch 5: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.fit(word_seq_train, y_train, batch_size=64, epochs=20, validation_split=0.1, shuffle=True, callbacks = callbacks_list)\n",
    "model.save('cnn1.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3da1e77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "862/862 [==============================] - 19s 21ms/step\n",
      "Accuracy is---> 87.03206616366802\n"
     ]
    }
   ],
   "source": [
    "#model = load_model('cnn1.h5')\n",
    "y_pred_proba = model.predict(word_seq_test)\n",
    "#print(y_pred_proba)\n",
    "#print('---------------')\n",
    "\n",
    "y_pred_class = np.round(y_pred_proba)\n",
    "y_pred_class = y_pred_class.squeeze() #makes y_pred_class 1 dim\n",
    "'''print(y_pred_class)\n",
    "print(type(y_pred_class))\n",
    "print(len(y_pred_class))\n",
    "print(len(y_test))\n",
    "print('---------------')'''\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred_class) * 100\n",
    "print(\"Accuracy is--->\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f7a78b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "226ae6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 26)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_4 (Embedding)        (None, 26, 300)      28156500    ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 26, 200)      180200      ['embedding_4[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 26, 200)      360200      ['embedding_4[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 26, 200)      540200      ['embedding_4[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 26, 200)      720200      ['embedding_4[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 104, 200)     0           ['conv1d_2[0][0]',               \n",
      "                                                                  'conv1d_3[0][0]',               \n",
      "                                                                  'conv1d_4[0][0]',               \n",
      "                                                                  'conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " global_max_pooling1d_1 (Global  (None, 200)         0           ['concatenate[0][0]']            \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 200)          0           ['global_max_pooling1d_1[0][0]'] \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 128)          25728       ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 64)           8256        ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 1)            65          ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 29,991,349\n",
      "Trainable params: 1,834,849\n",
      "Non-trainable params: 28,156,500\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#CNN2\n",
    "\n",
    "#num_classes = len(label_names)\n",
    "num_classes = 2\n",
    "\n",
    "inputt = Input(shape=(maxi,))\n",
    "model = Embedding(nb_words+1, embed_dim, weights=[embedding_matrix],input_length=maxi,trainable = False)(inputt)\n",
    "tower_1 = Conv1D(num_kernels, kernel_size_p1, padding='same', activation='relu', strides=stride)(model)\n",
    "tower_2 = Conv1D(num_kernels, kernel_size_p2, padding='same', activation='relu', strides=stride)(model)\n",
    "tower_3 = Conv1D(num_kernels, kernel_size_p3, padding='same', activation='relu', strides=stride)(model)\n",
    "tower_4 = Conv1D(num_kernels, kernel_size_p4, padding='same', activation='relu', strides=stride)(model)\n",
    "model = keras.layers.concatenate([tower_1, tower_2, tower_3, tower_4], axis=1)\n",
    "model = GlobalMaxPooling1D()(model)\n",
    "model = Dropout(0.5)(model)\n",
    "#model = Flatten()(model)\n",
    "model = Dense(128, activation='relu')(model)\n",
    "model = Dense(64, activation='relu')(model)\n",
    "#output = Dense(num_classes, activation='sigmoid')(model)\n",
    "output = Dense(1, activation='sigmoid')(model)\n",
    "model = Model(inputt, output)\n",
    "\n",
    "#adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d6bf9c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "905/905 [==============================] - 224s 243ms/step - loss: 0.6177 - accuracy: 0.6522 - val_loss: 0.5586 - val_accuracy: 0.7022\n",
      "Epoch 2/20\n",
      "905/905 [==============================] - 237s 262ms/step - loss: 0.5506 - accuracy: 0.7109 - val_loss: 0.5058 - val_accuracy: 0.7502\n",
      "Epoch 3/20\n",
      "905/905 [==============================] - 259s 286ms/step - loss: 0.5097 - accuracy: 0.7438 - val_loss: 0.4788 - val_accuracy: 0.7690\n",
      "Epoch 4/20\n",
      "905/905 [==============================] - 269s 297ms/step - loss: 0.4819 - accuracy: 0.7689 - val_loss: 0.4628 - val_accuracy: 0.7870\n",
      "Epoch 5/20\n",
      "905/905 [==============================] - 224s 247ms/step - loss: 0.4612 - accuracy: 0.7830 - val_loss: 0.4460 - val_accuracy: 0.7884\n",
      "Epoch 6/20\n",
      "905/905 [==============================] - 232s 256ms/step - loss: 0.4464 - accuracy: 0.7924 - val_loss: 0.4279 - val_accuracy: 0.8090\n",
      "Epoch 7/20\n",
      "905/905 [==============================] - 230s 254ms/step - loss: 0.4326 - accuracy: 0.8031 - val_loss: 0.4113 - val_accuracy: 0.8150\n",
      "Epoch 8/20\n",
      "905/905 [==============================] - 233s 257ms/step - loss: 0.4219 - accuracy: 0.8102 - val_loss: 0.4136 - val_accuracy: 0.8104\n",
      "Epoch 9/20\n",
      "905/905 [==============================] - 234s 258ms/step - loss: 0.4161 - accuracy: 0.8130 - val_loss: 0.4047 - val_accuracy: 0.8175\n",
      "Epoch 10/20\n",
      "905/905 [==============================] - 228s 252ms/step - loss: 0.4050 - accuracy: 0.8192 - val_loss: 0.4167 - val_accuracy: 0.8178\n",
      "Epoch 11/20\n",
      "905/905 [==============================] - 232s 256ms/step - loss: 0.3980 - accuracy: 0.8240 - val_loss: 0.4052 - val_accuracy: 0.8122\n",
      "Epoch 11: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.fit(word_seq_train, y_train, batch_size=64, epochs=20, validation_split=0.1, shuffle=True, callbacks = callbacks_list)\n",
    "model.save('cnn2.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6a68cf5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "862/862 [==============================] - 49s 54ms/step\n",
      "Accuracy is---> 81.95371445153802\n"
     ]
    }
   ],
   "source": [
    "#model = load_model('cnn2.h5')\n",
    "y_pred_proba = model.predict(word_seq_test)\n",
    "#print(y_pred_proba)\n",
    "#print('---------------')\n",
    "\n",
    "y_pred_class = np.round(y_pred_proba)\n",
    "y_pred_class = y_pred_class.squeeze() #makes y_pred_class 1 dim\n",
    "'''print(y_pred_class)\n",
    "print(type(y_pred_class))\n",
    "print(len(y_pred_class))\n",
    "print(len(y_test))\n",
    "print('---------------')'''\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred_class) * 100\n",
    "print(\"Accuracy is--->\", acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
