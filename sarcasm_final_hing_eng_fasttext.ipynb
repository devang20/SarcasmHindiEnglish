{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a42c092",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 14:46:26.906654: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-30 14:46:26.906748: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "[nltk_data] Downloading package punkt to /home/user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "from collections import Counter\n",
    "import emoji\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from gensim.models import FastText\n",
    "from gensim.models import KeyedVectors\n",
    "#from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "#from tqdm import tqdm\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "#import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.data_utils import pad_sequences\n",
    "#from keras.initializers import Constant\n",
    "from keras.layers import ReLU\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Embedding, Input, LSTM, Bidirectional, TimeDistributed, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras import Model\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import math\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6986cec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422415\n",
      "Index(['tweet_text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_hing_eng = pd.read_pickle('dataset/hinglish_english_tweet_ids_data.pkl')\n",
    "#print(df_hing_eng.columns)\n",
    "#print(df_hing_eng[['tweet_text']])\n",
    "#df_hing_eng = df_hing_eng['tweet_text']\n",
    "df_hing_eng = df_hing_eng[['tweet_text']].copy()\n",
    "print(len(df_hing_eng))\n",
    "print(df_hing_eng.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4455e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17695\n",
      "                                               tweet_text\n",
      "368     Chitti ai hai ai hai chitthi ai hai = I have m...\n",
      "372     Chitti ai hai ai hai chitthi ai hai = I have m...\n",
      "393                    Kim Kardashian is virgin. #sarcasm\n",
      "394                    Kim Kardashian is virgin. #sarcasm\n",
      "423                    Kim Kardashian is virgin. #sarcasm\n",
      "...                                                   ...\n",
      "422093                                                 hi\n",
      "422100                                   is going to bed.\n",
      "422121                                             nothin\n",
      "422141                                           wakes up\n",
      "422251                                      in the office\n",
      "\n",
      "[17695 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "#finds the duplicate rows in the unlabeled Hinglish-English data\n",
    "\n",
    "duplicate_1 = df_hing_eng[df_hing_eng.duplicated(keep='first')]\n",
    "print(len(duplicate_1))\n",
    "print(duplicate_1)\n",
    "#print(type(duplicate_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8035c4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404720\n",
      "0\n",
      "Index(['tweet_text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#removes duplicates\n",
    "\n",
    "df_hing_eng = df_hing_eng.drop_duplicates(keep='first')\n",
    "print(len(df_hing_eng))\n",
    "\n",
    "duplicate_2 = df_hing_eng[df_hing_eng.duplicated(keep='first')]\n",
    "print(len(duplicate_2))\n",
    "print(df_hing_eng.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4765ea9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94788\n",
      "Index(['tweet_text', 'is_sarcastic'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_labeled = pd.read_pickle('dataset/data_soumyadeep.pkl')\n",
    "df_labeled = df_labeled[['tweet_text', 'is_sarcastic']].copy()\n",
    "print(len(df_labeled))\n",
    "print(df_labeled.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb4caaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2895\n",
      "                                              tweet_text  is_sarcastic\n",
      "354    RT Apna vyauhar hoga aise jal mein kamal rehat...             0\n",
      "630    @fittaymunhapka n osky baad amMi ny kha k kuch...             0\n",
      "686    RT Teen behan ke baad jo aata putra woh trekha...             0\n",
      "809    Ek aadmi pareshan ho kar Ae bhagwan aisi zinda...             1\n",
      "844    RT Teen behan ke baad jo aata putra woh trekha...             0\n",
      "...                                                  ...           ...\n",
      "94716  RT Teen behan ke baad jo aata putra woh trekha...             0\n",
      "94721  Roz badh'taa huun jahaa'n se aage // Phir wahi...             0\n",
      "94747  RT Chhoota vaibhav schooli shiksha shooro ho g...             0\n",
      "94756                                kya hoga is desh ka             0\n",
      "94763  @rjauhari lol  unka bhi mast hoga..koi recessi...             0\n",
      "\n",
      "[2895 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#finds the duplicate rows in the labeled data\n",
    "\n",
    "duplicate_3 = df_labeled[df_labeled.duplicated(subset=['tweet_text'], keep='first')]\n",
    "print(len(duplicate_3))\n",
    "print(duplicate_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83886e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91893\n",
      "0\n",
      "Index(['tweet_text', 'is_sarcastic'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#removes duplicates from labeled data\n",
    "\n",
    "df_labeled = df_labeled.drop_duplicates(subset=['tweet_text'], keep='first')\n",
    "print(len(df_labeled))\n",
    "\n",
    "duplicate_4 = df_labeled[df_labeled.duplicated(keep='first')]\n",
    "print(len(duplicate_4))\n",
    "print(df_labeled.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1c8c61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              tweet_text  is_sarcastic\n",
      "0      @ajuonline #sarcasm Haal chaal thik thak hai-s...             1\n",
      "1               @I_Am_Hollywood - Charlie Haas? #sarcasm             1\n",
      "2                       Chetan Bhagat #epicmale #sarcasm             1\n",
      "3      @Bo_jasim86 LOOL 7atheer affaa 3allaik rgoobti...             1\n",
      "4      @FENUSF tees2al 3anek el 3afiaa 7abeeb galbe ,...             1\n",
      "...                                                  ...           ...\n",
      "54025   BHAI NEEND HEE NI ARAHI AKHIR KIA MAZAK HAI??)??             1\n",
      "54026  - Neend Toh Bachpan Mein Aaya Karti Thi..!! - ...             1\n",
      "54027  aik larki late night neend se jagi aur.. fb pe...             1\n",
      "54028  @FarahKhanAli really funny.. lagta hai.... sub...             1\n",
      "54029  @Rudeicious ab tum meri neend ka aise Mazak ur...             1\n",
      "\n",
      "[54030 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "54030"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finds the common rows between unlabeled Hinglish English embedding data and labeled data\n",
    "\n",
    "common_rows = pd.merge(df_hing_eng, df_labeled, how = 'inner', on=['tweet_text'])\n",
    "print(common_rows)\n",
    "len(common_rows)\n",
    "#print(type(common_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "937de183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350690\n",
      "Index(['tweet_text'], dtype='object')\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#removes all the common rows from unlabeled data\n",
    "\n",
    "#cond = df1['Email'].isin(df2['Email'])\n",
    "#df1.drop(df1[cond].index, inplace = True)\n",
    "\n",
    "common = df_hing_eng['tweet_text'].isin(df_labeled['tweet_text'])\n",
    "df_hing_eng.drop(df_hing_eng[common].index, inplace=True)\n",
    "print(len(df_hing_eng))\n",
    "print(df_hing_eng.columns)\n",
    "\n",
    "common_rows2 = pd.merge(df_hing_eng, df_labeled, how = 'inner', on=['tweet_text'])\n",
    "#print(common_rows2)\n",
    "print(len(common_rows2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df22e32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318723\n"
     ]
    }
   ],
   "source": [
    "#removes all rows from the Dataframe containing Devanagari script (any non-English word) \n",
    "\n",
    "df_hing_eng = df_hing_eng[df_hing_eng['tweet_text'].map(lambda x: x.isascii())]\n",
    "print(len(df_hing_eng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13d6dcb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6642/2924867879.py:6: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  source = source.str.replace(\"\\w*\\d\\w*\",\"\") #removes all words containing numbers\n",
      "/tmp/ipykernel_6642/2924867879.py:10: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  source = source.str.replace(\"[@A-Za-z0-9_]*@[@A-Za-z0-9_]*\",\"\")\n",
      "/tmp/ipykernel_6642/2924867879.py:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  source = source.str.replace(\"[#A-Za-z0-9_]*#[#A-Za-z0-9_]*\",\"\")\n",
      "/tmp/ipykernel_6642/2924867879.py:13: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  source = source.str.replace(\"\\s+|[!$%&()*+, -./:;<=>?\\^_`{|}~]\\s*\",\" \")\n",
      "/tmp/ipykernel_6642/2924867879.py:14: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  source = source.str.replace(\"http\\S+\",\"\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>dary emratiba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>yograj singh  dhoni advised dd management to r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>seedhe log hai ma beta ab  crore ka ghotala ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>ab ham khade ho gaye hain   ab ham aur bhi dar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>make up hayoona</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            tweet_text\n",
       "167                                      dary emratiba\n",
       "194  yograj singh  dhoni advised dd management to r...\n",
       "200  seedhe log hai ma beta ab  crore ka ghotala ho...\n",
       "309  ab ham khade ho gaye hain   ab ham aur bhi dar...\n",
       "470                                    make up hayoona"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removes mentions, '#', emojis, punctuations, special chars, URLs, words containing numbers, spaces from beginning and end of tweets\n",
    "\n",
    "def data_preprocessing(source):\n",
    "    \n",
    "    source = source.str.lower() #lower case chars\n",
    "    source = source.str.replace(\"\\w*\\d\\w*\",\"\") #removes all words containing numbers\n",
    "    #source = source.str.replace(\"#\", \"\")\n",
    "    #source = source.str.replace(\"@\", \"\")\n",
    "    #source = source.str.replace(\"@[A-Za-z0-9_]+\",\"\")\n",
    "    source = source.str.replace(\"[@A-Za-z0-9_]*@[@A-Za-z0-9_]*\",\"\")\n",
    "    #source = source.str.replace(\"#[A-Za-z0-9_]+\",\"\")\n",
    "    source = source.str.replace(\"[#A-Za-z0-9_]*#[#A-Za-z0-9_]*\",\"\")\n",
    "    source = source.str.replace(\"\\s+|[!$%&()*+, -./:;<=>?\\^_`{|}~]\\s*\",\" \")\n",
    "    source = source.str.replace(\"http\\S+\",\"\")\n",
    "    \n",
    "    return source\n",
    "\n",
    "df_hing_eng['tweet_text'] = data_preprocessing(df_hing_eng['tweet_text'])\n",
    "df_hing_eng['tweet_text'] = df_hing_eng['tweet_text'].apply(lambda s: emoji.replace_emoji(s,\"\"))\n",
    "df_hing_eng['tweet_text'] = df_hing_eng['tweet_text'].str.strip()\n",
    "df_hing_eng.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1067ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197446\n"
     ]
    }
   ],
   "source": [
    "#finds the unique words in the unlabeled data\n",
    "\n",
    "unique_words = set()\n",
    "df_hing_eng['tweet_text'].str.lower().str.split().apply(unique_words.update)\n",
    "unique_words = list(unique_words)\n",
    "#print(unique_words)\n",
    "#print('---------------')\n",
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e547f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>[dary, emratiba]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>[yograj, singh, dhoni, advised, dd, management...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>[seedhe, log, hai, ma, beta, ab, crore, ka, gh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>[ab, ham, khade, ho, gaye, hain, ab, ham, aur,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>[make, up, hayoona]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>[cut, marna, koi, umar, se, seekhay, nahi, see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>[woh, jo, kar, rahe, haen, woh, desh, sewa, ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>[u, r, aapne, chaata, nahi, ki, tarah, pehle, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1077</th>\n",
       "      <td>[bhai, tu, ek, vaio, mini, laptop, le, le]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>[aanaayi, janichirunengil, kashtapettu, cash, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             tweet_text\n",
       "167                                    [dary, emratiba]\n",
       "194   [yograj, singh, dhoni, advised, dd, management...\n",
       "200   [seedhe, log, hai, ma, beta, ab, crore, ka, gh...\n",
       "309   [ab, ham, khade, ho, gaye, hain, ab, ham, aur,...\n",
       "470                                 [make, up, hayoona]\n",
       "571   [cut, marna, koi, umar, se, seekhay, nahi, see...\n",
       "804   [woh, jo, kar, rahe, haen, woh, desh, sewa, ha...\n",
       "905   [u, r, aapne, chaata, nahi, ki, tarah, pehle, ...\n",
       "1077         [bhai, tu, ek, vaio, mini, laptop, le, le]\n",
       "1277  [aanaayi, janichirunengil, kashtapettu, cash, ..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generates tokenized data \n",
    "\n",
    "#nltk.download('punkt')\n",
    "df_hing_eng['tweet_text'] = df_hing_eng['tweet_text'].apply(nltk.word_tokenize)\n",
    "df_hing_eng.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01723012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>[dary, emratiba]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>[yograj, singh, dhoni, advised, dd, management...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>[seedhe, log, hai, ma, beta, ab, crore, ka, gh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>[ab, ham, khade, ho, gaye, hain, ab, ham, aur,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>[make, up, hayoona]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>[cut, marna, koi, umar, se, seekhay, nahi, see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>[woh, jo, kar, rahe, haen, woh, desh, sewa, ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>[u, r, aapne, chaata, nahi, ki, tarah, pehle, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1077</th>\n",
       "      <td>[bhai, tu, ek, vaio, mini, laptop, le, le]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>[aanaayi, janichirunengil, kashtapettu, cash, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             tweet_text\n",
       "167                                    [dary, emratiba]\n",
       "194   [yograj, singh, dhoni, advised, dd, management...\n",
       "200   [seedhe, log, hai, ma, beta, ab, crore, ka, gh...\n",
       "309   [ab, ham, khade, ho, gaye, hain, ab, ham, aur,...\n",
       "470                                 [make, up, hayoona]\n",
       "571   [cut, marna, koi, umar, se, seekhay, nahi, see...\n",
       "804   [woh, jo, kar, rahe, haen, woh, desh, sewa, ha...\n",
       "905   [u, r, aapne, chaata, nahi, ki, tarah, pehle, ...\n",
       "1077         [bhai, tu, ek, vaio, mini, laptop, le, le]\n",
       "1277  [aanaayi, janichirunengil, kashtapettu, cash, ..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removes all the rows with empty lists\n",
    "\n",
    "df_hing_eng=df_hing_eng[df_hing_eng['tweet_text'].map(lambda d: len(d)) > 0]\n",
    "df_hing_eng.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85c34e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding size---> 300\n"
     ]
    }
   ],
   "source": [
    "#model = Word2Vec(df_hing_eng['tweet_text'], vector_size=300, window=10, hs=0, negative = 1)\n",
    "model = FastText(df_hing_eng['tweet_text'], vector_size=300, window=10, hs=0, negative = 1)\n",
    "embedding_size = model.wv.vectors.shape[1]\n",
    "print(\"embedding size--->\", embedding_size)\n",
    "vocab = model.wv.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7365a764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!!!\n"
     ]
    }
   ],
   "source": [
    "#saves the FastText model\n",
    "\n",
    "model.save(\"hing_eng_fasttext.model\")\n",
    "print(\"Model saved!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0236440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=39876, vector_size=300, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = FastText.load(\"hing_eng_fasttext.model\")\n",
    "print(embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ea7799f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          tweet_text  is_sarcastic\n",
      "0  LOL !  BC badaa gaandu hai to ! so jaa lodu ! ...             1\n",
      "1  @OFFICIAL_FC_PAK apni sakh khud sambhalen aur ...             0\n",
      "2  Ek aadmi pareshan ho kar Ae bhagwan aisi zinda...             1\n",
      "3  Lol RT @ThePrachi: @shrikhande Who can sing a ...             1\n",
      "4  @idrinkblueblood tu ol's kar rahi ho kya? Kya ...             0\n",
      "91893\n"
     ]
    }
   ],
   "source": [
    "print(df_labeled.head())\n",
    "print(len(df_labeled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d933ca4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6642/3018622983.py:6: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  source = source.str.replace(\"\\w*\\d\\w*\",\"\") #removes all words containing numbers\n",
      "/tmp/ipykernel_6642/3018622983.py:10: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  source = source.str.replace(\"[@A-Za-z0-9_]*@[@A-Za-z0-9_]*\",\"\")\n",
      "/tmp/ipykernel_6642/3018622983.py:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  source = source.str.replace(\"[#A-Za-z0-9_]*#[#A-Za-z0-9_]*\",\"\")\n",
      "/tmp/ipykernel_6642/3018622983.py:13: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  source = source.str.replace(\"\\s+|[!$%&()*+, -./:;<=>?\\^_`{|}~]\\s*\",\" \")\n",
      "/tmp/ipykernel_6642/3018622983.py:14: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  source = source.str.replace(\"http\\S+\",\"\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lol  bc badaa gaandu hai to  so jaa lodu  tere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apni sakh khud sambhalen aur kisi ka saath na ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ek aadmi pareshan ho kar ae bhagwan aisi zinda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lol rt  who can sing a group song alone raavan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tu ol s kar rahi ho kya kya hoga tere kaam sir...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text\n",
       "0  lol  bc badaa gaandu hai to  so jaa lodu  tere...\n",
       "1  apni sakh khud sambhalen aur kisi ka saath na ...\n",
       "2  ek aadmi pareshan ho kar ae bhagwan aisi zinda...\n",
       "3  lol rt  who can sing a group song alone raavan...\n",
       "4  tu ol s kar rahi ho kya kya hoga tere kaam sir..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removes mentions, '#', emojis, punctuations, special chars, URLs, words containing numbers, spaces from beginning and end of tweets\n",
    "\n",
    "def data_preprocessing(source):\n",
    "    \n",
    "    source = source.str.lower() #lower case chars\n",
    "    source = source.str.replace(\"\\w*\\d\\w*\",\"\") #removes all words containing numbers\n",
    "    #source = source.str.replace(\"#\", \"\")\n",
    "    #source = source.str.replace(\"@\", \"\")\n",
    "    #source = source.str.replace(\"@[A-Za-z0-9_]+\",\"\")\n",
    "    source = source.str.replace(\"[@A-Za-z0-9_]*@[@A-Za-z0-9_]*\",\"\")\n",
    "    #source = source.str.replace(\"#[A-Za-z0-9_]+\",\"\")\n",
    "    source = source.str.replace(\"[#A-Za-z0-9_]*#[#A-Za-z0-9_]*\",\"\")\n",
    "    source = source.str.replace(\"\\s+|[!$%&()*+, -./:;<=>?\\^_`{|}~]\\s*\",\" \")\n",
    "    source = source.str.replace(\"http\\S+\",\"\")\n",
    "    \n",
    "    return source\n",
    "\n",
    "df_labeled['tweet_text'] = data_preprocessing(df_labeled['tweet_text'])\n",
    "df_labeled['tweet_text'] = df_labeled['tweet_text'].apply(lambda s: emoji.replace_emoji(s,\"\"))\n",
    "df_labeled['tweet_text'] = df_labeled['tweet_text'].str.strip()\n",
    "df_labeled[['tweet_text']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e177fc14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lol  bc badaa gaandu hai to  so jaa lodu  tere...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apni sakh khud sambhalen aur kisi ka saath na ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ek aadmi pareshan ho kar ae bhagwan aisi zinda...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lol rt  who can sing a group song alone raavan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tu ol s kar rahi ho kya kya hoga tere kaam sir...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  is_sarcastic\n",
       "0  lol  bc badaa gaandu hai to  so jaa lodu  tere...             1\n",
       "1  apni sakh khud sambhalen aur kisi ka saath na ...             0\n",
       "2  ek aadmi pareshan ho kar ae bhagwan aisi zinda...             1\n",
       "3  lol rt  who can sing a group song alone raavan...             1\n",
       "4  tu ol s kar rahi ho kya kya hoga tere kaam sir...             0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc091d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93958\n"
     ]
    }
   ],
   "source": [
    "#finds the unique words in the labeled data\n",
    "\n",
    "unique_words = set()\n",
    "df_labeled['tweet_text'].str.lower().str.split().apply(unique_words.update)\n",
    "unique_words = list(unique_words)\n",
    "#print(unique_words)\n",
    "#print('---------------')\n",
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76d9fb98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[lol, bc, badaa, gaandu, hai, to, so, jaa, lod...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[apni, sakh, khud, sambhalen, aur, kisi, ka, s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ek, aadmi, pareshan, ho, kar, ae, bhagwan, ai...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[lol, rt, who, can, sing, a, group, song, alon...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[tu, ol, s, kar, rahi, ho, kya, kya, hoga, ter...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  is_sarcastic\n",
       "0  [lol, bc, badaa, gaandu, hai, to, so, jaa, lod...             1\n",
       "1  [apni, sakh, khud, sambhalen, aur, kisi, ka, s...             0\n",
       "2  [ek, aadmi, pareshan, ho, kar, ae, bhagwan, ai...             1\n",
       "3  [lol, rt, who, can, sing, a, group, song, alon...             1\n",
       "4  [tu, ol, s, kar, rahi, ho, kya, kya, hoga, ter...             0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generates tokenized data \n",
    "\n",
    "#nltk.download('punkt')\n",
    "df_labeled['tweet_text'] = df_labeled['tweet_text'].apply(nltk.word_tokenize)\n",
    "df_labeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "514a8539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91893\n",
      "                                          tweet_text  is_sarcastic\n",
      "0  [lol, bc, badaa, gaandu, hai, to, so, jaa, lod...             1\n",
      "1  [apni, sakh, khud, sambhalen, aur, kisi, ka, s...             0\n",
      "2  [ek, aadmi, pareshan, ho, kar, ae, bhagwan, ai...             1\n",
      "3  [lol, rt, who, can, sing, a, group, song, alon...             1\n",
      "4  [tu, ol, s, kar, rahi, ho, kya, kya, hoga, ter...             0\n",
      "5      [yaar, ye, log, tu, serious, ho, gaye, t, co]             0\n",
      "6  [abey, tumhare, paas, kaam, nahi, hai, jo, twi...             0\n",
      "7  [ani, tauko, dukhyo, vanera, jaane, ta, ho, ni...             0\n",
      "8  [rt, agar, tumne, rt, nahi, kiya, to, mujhse, ...             0\n",
      "9  [dil, ki, dhadkan, sooni, sooni, saansain, bhi...             0\n",
      "91892\n"
     ]
    }
   ],
   "source": [
    "#removes all the rows with empty lists\n",
    "\n",
    "print(len(df_labeled))\n",
    "df_labeled = df_labeled[df_labeled['tweet_text'].map(lambda d: len(d)) > 0]\n",
    "print(df_labeled.head(10))\n",
    "print(len(df_labeled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "714b3af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[lol, bc, badaa, gaandu, hai, to, so, jaa, lod...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[apni, sakh, khud, sambhalen, aur, kisi, ka, s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ek, aadmi, pareshan, ho, kar, ae, bhagwan, ai...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[lol, rt, who, can, sing, a, group, song, alon...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[tu, ol, s, kar, rahi, ho, kya, kya, hoga, ter...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  is_sarcastic\n",
       "0  [lol, bc, badaa, gaandu, hai, to, so, jaa, lod...             1\n",
       "1  [apni, sakh, khud, sambhalen, aur, kisi, ka, s...             0\n",
       "2  [ek, aadmi, pareshan, ho, kar, ae, bhagwan, ai...             1\n",
       "3  [lol, rt, who, can, sing, a, group, song, alon...             1\n",
       "4  [tu, ol, s, kar, rahi, ho, kya, kya, hoga, ter...             0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "824f5f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>[dary, emratiba]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>[yograj, singh, dhoni, advised, dd, management...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>[seedhe, log, hai, ma, beta, ab, crore, ka, gh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>[ab, ham, khade, ho, gaye, hain, ab, ham, aur,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>[make, up, hayoona]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            tweet_text\n",
       "167                                   [dary, emratiba]\n",
       "194  [yograj, singh, dhoni, advised, dd, management...\n",
       "200  [seedhe, log, hai, ma, beta, ab, crore, ka, gh...\n",
       "309  [ab, ham, khade, ho, gaye, hain, ab, ham, aur,...\n",
       "470                                [make, up, hayoona]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hing_eng.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17bfec17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "x_train_shape---> (64324,)\n",
      "y_train_shape---> (64324,)\n",
      "x_test_shape---> (27568,)\n",
      "y_test_shape---> (27568,)\n",
      "<class 'pandas.core.series.Series'>\n",
      "contents of first label---> 1\n"
     ]
    }
   ],
   "source": [
    "#train test split of labeled data\n",
    "\n",
    "check_nan = df_labeled['is_sarcastic'].isnull().values.any()\n",
    "print(check_nan)\n",
    "check_na = df_labeled['is_sarcastic'].isna().values.any()\n",
    "print(check_na)\n",
    "check_nan_ = df_labeled['tweet_text'].isnull().values.any()\n",
    "print(check_nan_)\n",
    "check_na_ = df_labeled['tweet_text'].isna().values.any()\n",
    "print(check_na_)\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(df_labeled['tweet_text'], df_labeled['is_sarcastic'], test_size=0.30, random_state=42)\n",
    "#x_train,x_test,y_train,y_test=train_test_split(padded_tweets, sarcasm_labels, test_size=0.20, random_state=42)\n",
    "print(\"x_train_shape--->\", x_train.shape)\n",
    "print(\"y_train_shape--->\", y_train.shape)\n",
    "print(\"x_test_shape--->\", x_test.shape)\n",
    "print(\"y_test_shape--->\", y_test.shape)\n",
    "print(type(x_train))\n",
    "print(\"contents of first label--->\", y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "245b986a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39384    [kisi, siyaasat, daan, ki, taraah, tumhen, haa...\n",
      "70407    [agar, bharat, ko, hai, mahaan, banna, toh, bh...\n",
      "12711    [atal, bihari, vajpayee, and, nawaz, sharif, s...\n",
      "20467    [bara, shok, tha, unhain, mera, ayeshana, daik...\n",
      "15898    [wo, aap, ne, sunna, hee, hoga, goray, kaam, k...\n",
      "                               ...                        \n",
      "6316     [oye, yaar, tera, dimag, kharab, hai, kya, abh...\n",
      "56186    [mulke, halate, jung, mein, hai, mujhe, koi, s...\n",
      "79025    [mastar, bandya, mashi, aani, daasa, madhe, ka...\n",
      "865      [ek, kunwari, larki, pehli, baar, ek, larke, s...\n",
      "16017    [always, joking, jo, chij, kbhi, sudhar, nhi, ...\n",
      "Name: tweet_text, Length: 64324, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "04fe7746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    32482\n",
      "1    31842\n",
      "Name: is_sarcastic, dtype: int64\n",
      "--------------------------\n",
      "0    13962\n",
      "1    13606\n",
      "Name: is_sarcastic, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_train.value_counts())\n",
    "print('--------------------------')\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "113fda14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length---> 84\n",
      "Mean length---> 16.626049375038864\n",
      "Standard Deviation of length---> 9.42481679865071\n",
      "Maximum length of the sequence---> 26\n"
     ]
    }
   ],
   "source": [
    "length = []\n",
    "for row in x_train:\n",
    "    #print(row)\n",
    "    #print(len(row))\n",
    "    #count = count+1\n",
    "    #if len(row)>maxi:\n",
    "    #print(len(row))\n",
    "    length.append(len(row))\n",
    "        \n",
    "#print(length)\n",
    "print(\"Max length--->\", max(length))\n",
    "print(\"Mean length--->\", np.mean(length))\n",
    "print(\"Standard Deviation of length--->\", np.std(length))\n",
    "\n",
    "#maxi = max(length)\n",
    "maxi = np.round(np.mean(length) + np.std(length)).astype(int)\n",
    "print(\"Maximum length of the sequence--->\", maxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "078c33d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['kisi', 'siyaasat', 'daan', 'ki', 'taraah', 'tumhen', 'haasil', 'karne', 'k', 'liye', 'roz', 'naye', 'jalse', 'karta', 'hoon', 'apne', 'dil', 'men'], ['agar', 'bharat', 'ko', 'hai', 'mahaan', 'banna', 'toh', 'bhrasht', 'netaao', 'ko', 'hoga', 'hatna', 'aur', 'bhrashtachaar', 'ko', 'hoga', 'mitaana', 'yeh', 'kisi', 'ek', 'seh', 'na', 'hoga', 'pureh', 'jansamuday', 'ko', 'hoga', 'saath', 'nibhaana', 'happy', 'independence', 'day'], ['atal', 'bihari', 'vajpayee', 'and', 'nawaz', 'sharif', 'share', 'the', 'same', 'date', 'of', 'birth'], ['bara', 'shok', 'tha', 'unhain', 'mera', 'ayeshana', 'daiknay', 'ka', 'naddan', 'jab', 'daiki', 'meri', 'garabi', 'tu', 'rasta', 'badal', 'liya'], ['wo', 'aap', 'ne', 'sunna', 'hee', 'hoga', 'goray', 'kaam', 'ko', 'dimaag', 'mei', 'rakhtay', 'hain', 'aur', 'hehe', 'magar', 'pakistani', 'uss', 'ko', 'dimag', 'mei', 'aur', 'kaam', 'ko']]\n"
     ]
    }
   ],
   "source": [
    "total_docs=[]\n",
    "total_docs.extend(x_train)\n",
    "total_docs.extend(x_test)\n",
    "print(total_docs[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c69d0b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing input data...\n"
     ]
    }
   ],
   "source": [
    "print(\"tokenizing input data...\")\n",
    "MAX_NB_WORDS = 200000\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(total_docs)\n",
    "#tokenizer.fit_on_texts(x_train)\n",
    "word_seq_train = tokenizer.texts_to_sequences(x_train)\n",
    "word_seq_test = tokenizer.texts_to_sequences(x_test)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9e5c27a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64324\n",
      "[58, 17091, 5027, 2, 25830, 1972, 4437, 122, 11, 42, 391, 1217, 9317, 149, 163, 87, 13, 380]\n",
      "27568\n",
      "[695, 4, 113, 623, 14, 172, 16, 76773, 5277, 7, 113, 623]\n",
      "dictionary size:  93854\n"
     ]
    }
   ],
   "source": [
    "print(len(word_seq_train))\n",
    "print(word_seq_train[0])\n",
    "print(len(word_seq_test))\n",
    "print(word_seq_test[0])\n",
    "print(\"dictionary size: \", len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a4b745cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   58 17091  5027     2 25830  1972  4437   122    11    42   391  1217\n",
      "  9317   149   163    87    13   380     0     0     0     0     0     0\n",
      "     0     0]\n",
      "(64324, 26)\n",
      "(27568, 26)\n"
     ]
    }
   ],
   "source": [
    "#pads to the length of the longest sequence\n",
    "\n",
    "word_seq_train = pad_sequences(word_seq_train, maxlen=maxi, padding='post')\n",
    "word_seq_test = pad_sequences(word_seq_test, maxlen=maxi, padding='post')\n",
    "print(word_seq_train[0])\n",
    "print(word_seq_train.shape)\n",
    "print(word_seq_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0b31fea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing embedding matrix...\n",
      "number of null word embeddings: 0\n",
      "(93855, 300)\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "#builds the embedding matrix\n",
    "\n",
    "print('preparing embedding matrix...')\n",
    "embed_dim = embedding_size\n",
    "words_not_found = []\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.random.rand(nb_words+1, embed_dim)\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    #print(word)\n",
    "\n",
    "    if embeddings_index.wv.__contains__(word): \n",
    "\n",
    "        embedding_vector = embeddings_index.wv[word]\n",
    "    \n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "#print(embedding_matrix)\n",
    "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "print(embedding_matrix.shape)\n",
    "print(len(embedding_matrix[34]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "359a0c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(words_not_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "36710787",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
    "callbacks_list = [early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "36201c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attention BiLSTM\n",
    "\n",
    "from keras import backend as K\n",
    "#from keras.engine.topology import Layer\n",
    "from tensorflow.keras.layers import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight( shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "        #print(self.name)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "72b778ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 15:16:23.947246: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-04-30 15:16:23.972480: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-04-30 15:16:24.019975: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (user-PC): /proc/driver/nvidia/version does not exist\n",
      "2023-04-30 15:16:24.280222: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-30 15:16:29.343299: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 112626000 exceeds 10% of free system memory.\n",
      "2023-04-30 15:16:31.694283: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 112626000 exceeds 10% of free system memory.\n",
      "2023-04-30 15:16:32.002419: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 112626000 exceeds 10% of free system memory.\n",
      "2023-04-30 15:16:33.843350: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 112626000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 26)]              0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 26, 300)           28156500  \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 26, 300)          541200    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " attention (Attention)       (None, 300)               326       \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               77056     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,775,339\n",
      "Trainable params: 618,839\n",
      "Non-trainable params: 28,156,500\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "lstm_out1 = 150\n",
    "#bilstm_out1 = 96\n",
    "#lstm_out2 = 32\n",
    "#dropout = 0.2\n",
    "#recurrent_dropout = 0.2\n",
    "\n",
    "inp = Input(shape=(maxi, ))\n",
    "x = Embedding(nb_words+1, embed_dim, weights=[embedding_matrix], trainable=False)(inp)\n",
    "x = Bidirectional(LSTM(lstm_out1, return_sequences=True, dropout=0.20, recurrent_dropout=0.20))(x)\n",
    "x = Attention(maxi)(x)\n",
    "x = Dense(256, activation=\"relu\")(x)\n",
    "#x = Dropout(0.25)(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a8749ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "905/905 [==============================] - 285s 303ms/step - loss: 0.2548 - accuracy: 0.9118 - val_loss: 0.1667 - val_accuracy: 0.9532\n",
      "Epoch 2/20\n",
      "905/905 [==============================] - 271s 299ms/step - loss: 0.1597 - accuracy: 0.9517 - val_loss: 0.1596 - val_accuracy: 0.9520\n",
      "Epoch 3/20\n",
      "905/905 [==============================] - 271s 300ms/step - loss: 0.1451 - accuracy: 0.9546 - val_loss: 0.1497 - val_accuracy: 0.9541\n",
      "Epoch 4/20\n",
      "905/905 [==============================] - 269s 297ms/step - loss: 0.1315 - accuracy: 0.9570 - val_loss: 0.1499 - val_accuracy: 0.9534\n",
      "Epoch 5/20\n",
      "905/905 [==============================] - 271s 300ms/step - loss: 0.1165 - accuracy: 0.9608 - val_loss: 0.1523 - val_accuracy: 0.9543\n",
      "Epoch 6/20\n",
      "905/905 [==============================] - 303s 335ms/step - loss: 0.1008 - accuracy: 0.9656 - val_loss: 0.1544 - val_accuracy: 0.9568\n",
      "Epoch 7/20\n",
      "905/905 [==============================] - 307s 339ms/step - loss: 0.0875 - accuracy: 0.9696 - val_loss: 0.1797 - val_accuracy: 0.9457\n",
      "Epoch 7: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5439791d30>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#trains attention BiLSTM model \n",
    "\n",
    "model.fit(word_seq_train, y_train, batch_size=64, epochs=20, validation_split=0.1, shuffle=True, callbacks = callbacks_list)\n",
    "#model.save('att_bilstm.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0e3a109e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "862/862 [==============================] - 52s 60ms/step\n",
      "Accuracy is---> 95.09939059779454\n"
     ]
    }
   ],
   "source": [
    "#model = load_model('att_bilstm.h5')\n",
    "y_pred_proba = model.predict(word_seq_test)\n",
    "#print(y_pred_proba)\n",
    "#print('---------------')\n",
    "\n",
    "y_pred_class = np.round(y_pred_proba)\n",
    "y_pred_class = y_pred_class.squeeze() #makes y_pred_class 1 dim\n",
    "'''print(y_pred_class)\n",
    "print(type(y_pred_class))\n",
    "print(len(y_pred_class))\n",
    "print(len(y_test))\n",
    "print('---------------')'''\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred_class) * 100\n",
    "print(\"Accuracy is--->\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cb7cc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab408829",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 19:42:30.398971: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 112626000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 26, 300)           28156500  \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 150)               270600    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                9664      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,436,829\n",
      "Trainable params: 280,329\n",
      "Non-trainable params: 28,156,500\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#this one--->LSTM\n",
    "\n",
    "lstm_out1 = 150\n",
    "'''bilstm_out1 = 96\n",
    "lstm_out2 = 32\n",
    "dropout = 0.2\n",
    "recurrent_dropout = 0.2'''\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_words+1, embed_dim, weights=[embedding_matrix], input_length=maxi, trainable=False))\n",
    "model.add(LSTM(lstm_out1, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f95f8ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "905/905 [==============================] - 186s 197ms/step - loss: 0.3442 - accuracy: 0.8596 - val_loss: 0.2442 - val_accuracy: 0.9179\n",
      "Epoch 2/20\n",
      "905/905 [==============================] - 137s 152ms/step - loss: 0.2036 - accuracy: 0.9361 - val_loss: 0.1796 - val_accuracy: 0.9484\n",
      "Epoch 3/20\n",
      "905/905 [==============================] - 131s 145ms/step - loss: 0.1705 - accuracy: 0.9484 - val_loss: 0.1677 - val_accuracy: 0.9510\n",
      "Epoch 4/20\n",
      "905/905 [==============================] - 129s 143ms/step - loss: 0.1570 - accuracy: 0.9515 - val_loss: 0.1605 - val_accuracy: 0.9531\n",
      "Epoch 5/20\n",
      "905/905 [==============================] - 145s 161ms/step - loss: 0.1454 - accuracy: 0.9540 - val_loss: 0.1616 - val_accuracy: 0.9517\n",
      "Epoch 6/20\n",
      "905/905 [==============================] - 191s 211ms/step - loss: 0.1390 - accuracy: 0.9556 - val_loss: 0.1618 - val_accuracy: 0.9527\n",
      "Epoch 7/20\n",
      "905/905 [==============================] - 137s 152ms/step - loss: 0.1293 - accuracy: 0.9578 - val_loss: 0.1651 - val_accuracy: 0.9517\n",
      "Epoch 7: early stopping\n"
     ]
    }
   ],
   "source": [
    "#trains the LSTM model\n",
    "\n",
    "model.fit(word_seq_train, y_train, batch_size=64, epochs=20, validation_split=0.1, shuffle=True, callbacks = callbacks_list)\n",
    "model.save('lstm.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9c913b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "862/862 [==============================] - 26s 30ms/step\n",
      "Accuracy is---> 95.14654672083576\n"
     ]
    }
   ],
   "source": [
    "#model = load_model('att_bilstm.h5')\n",
    "y_pred_proba = model.predict(word_seq_test)\n",
    "#print(y_pred_proba)\n",
    "#print('---------------')\n",
    "\n",
    "y_pred_class = np.round(y_pred_proba)\n",
    "y_pred_class = y_pred_class.squeeze() #makes y_pred_class 1 dim\n",
    "'''print(y_pred_class)\n",
    "print(type(y_pred_class))\n",
    "print(len(y_pred_class))\n",
    "print(len(y_test))\n",
    "print('---------------')'''\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred_class) * 100\n",
    "print(\"Accuracy is--->\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9fdca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ca08d179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 26)]              0         \n",
      "                                                                 \n",
      " embedding_2 (Embedding)     (None, 26, 300)           28156500  \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 26, 300)          541200    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 26, 10)           3010      \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 260)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 64)                16704     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,717,479\n",
      "Trainable params: 560,979\n",
      "Non-trainable params: 28,156,500\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# BiLSTM\n",
    "\n",
    "lstm_out1 = 150\n",
    "bilstm_out1 = 96\n",
    "lstm_out2 = 32\n",
    "dropout = 0.2\n",
    "recurrent_dropout = 0.2\n",
    "\n",
    "# main model\n",
    "inputt = Input(shape=(maxi,))\n",
    "model = Embedding(nb_words+1, embed_dim, weights=[embedding_matrix],input_length=maxi,trainable = False)(inputt)\n",
    "model =  Bidirectional (LSTM (lstm_out1, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode='concat')(model)\n",
    "model = TimeDistributed(Dense(10,activation='relu'))(model)\n",
    "model = Flatten()(model)\n",
    "model = Dense(64,activation='relu')(model)\n",
    "output = Dense(1,activation='sigmoid')(model)\n",
    "model = Model(inputt,output)\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "89d4d33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "905/905 [==============================] - 333s 289ms/step - loss: 0.3091 - accuracy: 0.8817 - val_loss: 0.1970 - val_accuracy: 0.9422\n",
      "Epoch 2/20\n",
      "905/905 [==============================] - 391s 432ms/step - loss: 0.1715 - accuracy: 0.9486 - val_loss: 0.1616 - val_accuracy: 0.9523\n",
      "Epoch 3/20\n",
      "905/905 [==============================] - 318s 352ms/step - loss: 0.1437 - accuracy: 0.9547 - val_loss: 0.1470 - val_accuracy: 0.9554\n",
      "Epoch 4/20\n",
      "905/905 [==============================] - 346s 382ms/step - loss: 0.1243 - accuracy: 0.9591 - val_loss: 0.1440 - val_accuracy: 0.9543\n",
      "Epoch 5/20\n",
      "905/905 [==============================] - 304s 336ms/step - loss: 0.1089 - accuracy: 0.9636 - val_loss: 0.1535 - val_accuracy: 0.9546\n",
      "Epoch 6/20\n",
      "905/905 [==============================] - 334s 369ms/step - loss: 0.0947 - accuracy: 0.9678 - val_loss: 0.1477 - val_accuracy: 0.9569\n",
      "Epoch 7/20\n",
      "905/905 [==============================] - 343s 379ms/step - loss: 0.0817 - accuracy: 0.9716 - val_loss: 0.1601 - val_accuracy: 0.9538\n",
      "Epoch 7: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.fit(word_seq_train, y_train, batch_size=64, epochs=20, validation_split=0.1, shuffle=True, callbacks = callbacks_list)\n",
    "model.save('bilstm.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5972c217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "862/862 [==============================] - 63s 71ms/step\n",
      "Accuracy is---> 95.3786999419617\n"
     ]
    }
   ],
   "source": [
    "#model = load_model('att_bilstm.h5')\n",
    "y_pred_proba = model.predict(word_seq_test)\n",
    "#print(y_pred_proba)\n",
    "#print('---------------')\n",
    "\n",
    "y_pred_class = np.round(y_pred_proba)\n",
    "y_pred_class = y_pred_class.squeeze() #makes y_pred_class 1 dim\n",
    "'''print(y_pred_class)\n",
    "print(type(y_pred_class))\n",
    "print(len(y_pred_class))\n",
    "print(len(y_test))\n",
    "print('---------------')'''\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred_class) * 100\n",
    "print(\"Accuracy is--->\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0555ca53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ebb01112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training CNN ...\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 26, 300)           28156500  \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 26, 200)           420200    \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 13, 200)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 13, 200)           280200    \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 6, 200)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 200)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 200)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 128)               25728     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,890,949\n",
      "Trainable params: 734,449\n",
      "Non-trainable params: 28,156,500\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#CNN1\n",
    "\n",
    "#model parameters\n",
    "\n",
    "num_kernels=200\n",
    "stride=1\n",
    "embed_dim = 300\n",
    "weight_decay = 1e-4\n",
    "\n",
    "#parallel layers \n",
    "kernel_size=7\n",
    "kernel_size_p1=3\n",
    "kernel_size_p2=6\n",
    "kernel_size_p3=9\n",
    "kernel_size_p4=12\n",
    "\n",
    "#CNN architecture\n",
    "print(\"training CNN ...\")\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_words+1, embed_dim, weights=[embedding_matrix], input_length=maxi, trainable=False))\n",
    "model.add(Conv1D(num_kernels, kernel_size, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Conv1D(num_kernels, kernel_size, activation='relu', padding='same'))\n",
    "#model.add(GlobalMaxPooling1D())\n",
    "model.add(MaxPooling1D(2))\n",
    "#model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "#model.add(Dense(num_classes, activation='sigmoid'))  #multi-label (k-hot encoding)\n",
    "model.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4ec9f31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "905/905 [==============================] - 162s 177ms/step - loss: 0.4376 - accuracy: 0.8060 - val_loss: 0.3060 - val_accuracy: 0.8899\n",
      "Epoch 2/20\n",
      "905/905 [==============================] - 154s 170ms/step - loss: 0.3019 - accuracy: 0.8935 - val_loss: 0.2772 - val_accuracy: 0.9060\n",
      "Epoch 3/20\n",
      "905/905 [==============================] - 146s 161ms/step - loss: 0.2747 - accuracy: 0.9057 - val_loss: 0.2799 - val_accuracy: 0.9018\n",
      "Epoch 4/20\n",
      "905/905 [==============================] - 108s 120ms/step - loss: 0.2538 - accuracy: 0.9124 - val_loss: 0.2696 - val_accuracy: 0.9035\n",
      "Epoch 5/20\n",
      "905/905 [==============================] - 111s 123ms/step - loss: 0.2387 - accuracy: 0.9168 - val_loss: 0.2749 - val_accuracy: 0.8997\n",
      "Epoch 6/20\n",
      "905/905 [==============================] - 112s 124ms/step - loss: 0.2230 - accuracy: 0.9229 - val_loss: 0.2644 - val_accuracy: 0.9081\n",
      "Epoch 7/20\n",
      "905/905 [==============================] - 112s 124ms/step - loss: 0.2055 - accuracy: 0.9284 - val_loss: 0.2586 - val_accuracy: 0.9137\n",
      "Epoch 8/20\n",
      "905/905 [==============================] - 112s 123ms/step - loss: 0.1904 - accuracy: 0.9335 - val_loss: 0.2766 - val_accuracy: 0.9036\n",
      "Epoch 9/20\n",
      "905/905 [==============================] - 111s 122ms/step - loss: 0.1722 - accuracy: 0.9397 - val_loss: 0.2936 - val_accuracy: 0.8996\n",
      "Epoch 10/20\n",
      "905/905 [==============================] - 105s 116ms/step - loss: 0.1561 - accuracy: 0.9452 - val_loss: 0.3074 - val_accuracy: 0.9112\n",
      "Epoch 10: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.fit(word_seq_train, y_train, batch_size=64, epochs=20, validation_split=0.1, shuffle=True, callbacks = callbacks_list)\n",
    "model.save('cnn1.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4115a154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "862/862 [==============================] - 19s 21ms/step\n",
      "Accuracy is---> 91.52278003482299\n"
     ]
    }
   ],
   "source": [
    "#model = load_model('att_bilstm.h5')\n",
    "y_pred_proba = model.predict(word_seq_test)\n",
    "#print(y_pred_proba)\n",
    "#print('---------------')\n",
    "\n",
    "y_pred_class = np.round(y_pred_proba)\n",
    "y_pred_class = y_pred_class.squeeze() #makes y_pred_class 1 dim\n",
    "'''print(y_pred_class)\n",
    "print(type(y_pred_class))\n",
    "print(len(y_pred_class))\n",
    "print(len(y_test))\n",
    "print('---------------')'''\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred_class) * 100\n",
    "print(\"Accuracy is--->\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0efa37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fff919e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 26)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_4 (Embedding)        (None, 26, 300)      28156500    ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 26, 200)      180200      ['embedding_4[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 26, 200)      360200      ['embedding_4[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 26, 200)      540200      ['embedding_4[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 26, 200)      720200      ['embedding_4[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 104, 200)     0           ['conv1d_2[0][0]',               \n",
      "                                                                  'conv1d_3[0][0]',               \n",
      "                                                                  'conv1d_4[0][0]',               \n",
      "                                                                  'conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " global_max_pooling1d_1 (Global  (None, 200)         0           ['concatenate[0][0]']            \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 200)          0           ['global_max_pooling1d_1[0][0]'] \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 128)          25728       ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 64)           8256        ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 1)            65          ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 29,991,349\n",
      "Trainable params: 1,834,849\n",
      "Non-trainable params: 28,156,500\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#CNN2\n",
    "\n",
    "#num_classes = len(label_names)\n",
    "num_classes = 2\n",
    "\n",
    "inputt = Input(shape=(maxi,))\n",
    "model = Embedding(nb_words+1, embed_dim, weights=[embedding_matrix],input_length=maxi,trainable = False)(inputt)\n",
    "tower_1 = Conv1D(num_kernels, kernel_size_p1, padding='same', activation='relu', strides=stride)(model)\n",
    "tower_2 = Conv1D(num_kernels, kernel_size_p2, padding='same', activation='relu', strides=stride)(model)\n",
    "tower_3 = Conv1D(num_kernels, kernel_size_p3, padding='same', activation='relu', strides=stride)(model)\n",
    "tower_4 = Conv1D(num_kernels, kernel_size_p4, padding='same', activation='relu', strides=stride)(model)\n",
    "model = keras.layers.concatenate([tower_1, tower_2, tower_3, tower_4], axis=1)\n",
    "model = GlobalMaxPooling1D()(model)\n",
    "model = Dropout(0.5)(model)\n",
    "#model = Flatten()(model)\n",
    "model = Dense(128, activation='relu')(model)\n",
    "model = Dense(64, activation='relu')(model)\n",
    "#output = Dense(num_classes, activation='sigmoid')(model)\n",
    "output = Dense(1, activation='sigmoid')(model)\n",
    "model = Model(inputt, output)\n",
    "\n",
    "#adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "33be56ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "905/905 [==============================] - 262s 287ms/step - loss: 0.5309 - accuracy: 0.7439 - val_loss: 0.3663 - val_accuracy: 0.8540\n",
      "Epoch 2/20\n",
      "905/905 [==============================] - 336s 371ms/step - loss: 0.3650 - accuracy: 0.8576 - val_loss: 0.3019 - val_accuracy: 0.8952\n",
      "Epoch 3/20\n",
      "905/905 [==============================] - 297s 328ms/step - loss: 0.3196 - accuracy: 0.8814 - val_loss: 0.2917 - val_accuracy: 0.8921\n",
      "Epoch 4/20\n",
      "905/905 [==============================] - 254s 280ms/step - loss: 0.2995 - accuracy: 0.8900 - val_loss: 0.2741 - val_accuracy: 0.9066\n",
      "Epoch 5/20\n",
      "905/905 [==============================] - 297s 328ms/step - loss: 0.2826 - accuracy: 0.8976 - val_loss: 0.2662 - val_accuracy: 0.9081\n",
      "Epoch 6/20\n",
      "905/905 [==============================] - 264s 291ms/step - loss: 0.2704 - accuracy: 0.9040 - val_loss: 0.2564 - val_accuracy: 0.9102\n",
      "Epoch 7/20\n",
      "905/905 [==============================] - 226s 249ms/step - loss: 0.2589 - accuracy: 0.9074 - val_loss: 0.2548 - val_accuracy: 0.9126\n",
      "Epoch 8/20\n",
      "905/905 [==============================] - 237s 261ms/step - loss: 0.2478 - accuracy: 0.9109 - val_loss: 0.2583 - val_accuracy: 0.9074\n",
      "Epoch 9/20\n",
      "905/905 [==============================] - 233s 257ms/step - loss: 0.2373 - accuracy: 0.9140 - val_loss: 0.2408 - val_accuracy: 0.9176\n",
      "Epoch 10/20\n",
      "905/905 [==============================] - 232s 256ms/step - loss: 0.2256 - accuracy: 0.9176 - val_loss: 0.2394 - val_accuracy: 0.9175\n",
      "Epoch 11/20\n",
      "905/905 [==============================] - 233s 258ms/step - loss: 0.2154 - accuracy: 0.9212 - val_loss: 0.2336 - val_accuracy: 0.9226\n",
      "Epoch 12/20\n",
      "905/905 [==============================] - 248s 274ms/step - loss: 0.2032 - accuracy: 0.9249 - val_loss: 0.2445 - val_accuracy: 0.9184\n",
      "Epoch 13/20\n",
      "905/905 [==============================] - 249s 275ms/step - loss: 0.1942 - accuracy: 0.9279 - val_loss: 0.2428 - val_accuracy: 0.9196\n",
      "Epoch 13: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.fit(word_seq_train, y_train, batch_size=64, epochs=20, validation_split=0.1, shuffle=True, callbacks = callbacks_list)\n",
    "model.save('cnn2.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d13d7050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "862/862 [==============================] - 54s 62ms/step\n",
      "Accuracy is---> 92.04875217643644\n"
     ]
    }
   ],
   "source": [
    "#model = load_model('att_bilstm.h5')\n",
    "y_pred_proba = model.predict(word_seq_test)\n",
    "#print(y_pred_proba)\n",
    "#print('---------------')\n",
    "\n",
    "y_pred_class = np.round(y_pred_proba)\n",
    "y_pred_class = y_pred_class.squeeze() #makes y_pred_class 1 dim\n",
    "'''print(y_pred_class)\n",
    "print(type(y_pred_class))\n",
    "print(len(y_pred_class))\n",
    "print(len(y_test))\n",
    "print('---------------')'''\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred_class) * 100\n",
    "print(\"Accuracy is--->\", acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
